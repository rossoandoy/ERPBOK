# ERPçŸ¥è­˜RAGã‚·ã‚¹ãƒ†ãƒ  - Phase1 æŠ€è¡“ä»•æ§˜æ›¸

---
doc_type: "phase1_technical_specification"
complexity: "high"
estimated_effort: "æŠ€è¡“å®Ÿè£…ã®è©³ç´°ä»•æ§˜"
prerequisites: ["02_SystemArchitecture.md", "05_DataModelDesign.md", "16_Phase1ImplementationPlan.md"]
implementation_priority: "critical"
ai_assistance_level: "full_automation_possible"
version: "1.0.0"
author: "Claude Code"
created_date: "2025-01-21"
status: "approved"
approval_authority: "Technical Lead"
---

## ğŸ“‹ Phase1 æŠ€è¡“ä»•æ§˜æ¦‚è¦

### MVPæŠ€è¡“ä»•æ§˜ã®ç¯„å›²ãƒ»ç›®çš„
æœ¬æ–‡æ›¸ã¯ã€ŒERPçŸ¥è­˜RAGã‚·ã‚¹ãƒ†ãƒ ï¼ˆERPFTSï¼‰ã€Phase1 MVPã®è©³ç´°æŠ€è¡“å®Ÿè£…ä»•æ§˜ã‚’å®šç¾©ã™ã‚‹ã€‚æ–‡æ›¸å–ã‚Šè¾¼ã¿ã€ã‚»ãƒãƒ³ãƒ†ã‚£ãƒƒã‚¯æ¤œç´¢ã€WebUI ã®æ ¸å¿ƒæ©Ÿèƒ½ã«ã¤ã„ã¦ã€å®Ÿè£…ãƒ¬ãƒ™ãƒ«ã§ã®å…·ä½“çš„æŠ€è¡“ä»•æ§˜ãƒ»APIãƒ»ãƒ‡ãƒ¼ã‚¿æ§‹é€ ãƒ»ã‚¢ãƒ«ã‚´ãƒªã‚ºãƒ ã‚’è¦å®šã™ã‚‹ã€‚

### Phase1 æŠ€è¡“ã‚¢ãƒ¼ã‚­ãƒ†ã‚¯ãƒãƒ£æ¦‚è¦
```mermaid
graph TB
    subgraph "Phase1 MVP Architecture"
        UI[Streamlit WebUI]
        API[FastAPI REST API]
        
        subgraph "Core Services"
            DI[Document Ingestion]
            TE[Text Extraction]
            TP[Text Processing]
            ES[Embedding Service]
            SS[Search Service]
        end
        
        subgraph "Data Layer"
            SQLite[(SQLite DB)]
            Chroma[(Chroma Vector DB)]
            FS[File Storage]
        end
        
        UI --> API
        API --> DI
        API --> SS
        DI --> TE
        TE --> TP
        TP --> ES
        ES --> Chroma
        SS --> Chroma
        DI --> SQLite
        SS --> SQLite
        TE --> FS
    end
```

### Phase1 æŠ€è¡“ã‚¹ã‚¿ãƒƒã‚¯
```yaml
Language & Runtime:
  - Python 3.11+
  - AsyncIO (éåŒæœŸå‡¦ç†)
  - Type Hints (mypyæº–æ‹ )

Core Libraries:
  Document Processing:
    - PyPDF2 3.0+ (PDFæŠ½å‡º)
    - pdfplumber 0.9+ (PDFæ§‹é€ è§£æ)
    - BeautifulSoup4 4.12+ (HTMLè§£æ)
    - feedparser 6.0+ (RSSè§£æ)
    - python-docx (Wordæ–‡æ›¸å¯¾å¿œ)
  
  Text Processing:
    - spaCy 3.7+ (å¤šè¨€èªNLP)
    - langdetect 1.0+ (è¨€èªæ¤œå‡º)
    - sentence-transformers 2.2+ (åŸ‹ã‚è¾¼ã¿)
    - tiktoken 0.5+ (ãƒˆãƒ¼ã‚¯ãƒ³è¨ˆç®—)
  
  Vector Search:
    - chromadb 0.4+ (ãƒ™ã‚¯ãƒˆãƒ«DB)
    - sentence-transformers (åŸ‹ã‚è¾¼ã¿ãƒ¢ãƒ‡ãƒ«)
  
  Web Framework:
    - FastAPI 0.104+ (REST API)
    - Streamlit 1.28+ (WebUI)
    - Pydantic 2.5+ (ãƒ‡ãƒ¼ã‚¿æ¤œè¨¼)
  
  Data & Storage:
    - SQLAlchemy 2.0+ (ORM)
    - SQLite 3.35+ (ãƒ¡ã‚¿ãƒ‡ãƒ¼ã‚¿DB)
    - Alembic 1.12+ (ãƒã‚¤ã‚°ãƒ¬ãƒ¼ã‚·ãƒ§ãƒ³)

Development & Testing:
  - pytest 7.4+ (ãƒ†ã‚¹ãƒˆãƒ•ãƒ¬ãƒ¼ãƒ ãƒ¯ãƒ¼ã‚¯)
  - pytest-asyncio (éåŒæœŸãƒ†ã‚¹ãƒˆ)
  - Black (ã‚³ãƒ¼ãƒ‰ãƒ•ã‚©ãƒ¼ãƒãƒƒã‚¿)
  - mypy (å‹ãƒã‚§ãƒƒã‚¯)
  - pre-commit (Git hooks)
```

## ğŸ—‚ï¸ Phase1 ãƒ—ãƒ­ã‚¸ã‚§ã‚¯ãƒˆæ§‹é€ 

### ãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒªæ§‹æˆ
```
erpfts/
â”œâ”€â”€ pyproject.toml              # ãƒ—ãƒ­ã‚¸ã‚§ã‚¯ãƒˆè¨­å®šãƒ»ä¾å­˜é–¢ä¿‚
â”œâ”€â”€ requirements.txt            # Pythonä¾å­˜ãƒ©ã‚¤ãƒ–ãƒ©ãƒª
â”œâ”€â”€ .env.template              # ç’°å¢ƒå¤‰æ•°ãƒ†ãƒ³ãƒ—ãƒ¬ãƒ¼ãƒˆ
â”œâ”€â”€ README.md                  # ãƒ—ãƒ­ã‚¸ã‚§ã‚¯ãƒˆæ¦‚è¦ãƒ»ã‚»ãƒƒãƒˆã‚¢ãƒƒãƒ—
â”œâ”€â”€ Makefile                   # é–‹ç™ºç”¨ã‚³ãƒãƒ³ãƒ‰é›†ç´„
â”‚
â”œâ”€â”€ erpfts/                    # ãƒ¡ã‚¤ãƒ³ã‚¢ãƒ—ãƒªã‚±ãƒ¼ã‚·ãƒ§ãƒ³
â”‚   â”œâ”€â”€ __init__.py
â”‚   â”œâ”€â”€ main.py                # FastAPI ã‚¢ãƒ—ãƒªã‚±ãƒ¼ã‚·ãƒ§ãƒ³
â”‚   â”œâ”€â”€ cli.py                 # CLI ã‚³ãƒãƒ³ãƒ‰ã‚¨ãƒ³ãƒˆãƒªãƒ¼ãƒã‚¤ãƒ³ãƒˆ
â”‚   â”‚
â”‚   â”œâ”€â”€ core/                  # å…±é€šåŸºç›¤ãƒ»è¨­å®š
â”‚   â”‚   â”œâ”€â”€ __init__.py
â”‚   â”‚   â”œâ”€â”€ config.py          # è¨­å®šç®¡ç† (Pydantic Settings)
â”‚   â”‚   â”œâ”€â”€ database.py        # ãƒ‡ãƒ¼ã‚¿ãƒ™ãƒ¼ã‚¹æ¥ç¶šãƒ»ã‚»ãƒƒã‚·ãƒ§ãƒ³
â”‚   â”‚   â”œâ”€â”€ logging.py         # ãƒ­ã‚°è¨­å®šãƒ»ãƒãƒ³ãƒ‰ãƒ©ãƒ¼
â”‚   â”‚   â”œâ”€â”€ exceptions.py      # ã‚«ã‚¹ã‚¿ãƒ ä¾‹å¤–ã‚¯ãƒ©ã‚¹
â”‚   â”‚   â””â”€â”€ utils.py           # å…±é€šãƒ¦ãƒ¼ãƒ†ã‚£ãƒªãƒ†ã‚£
â”‚   â”‚
â”‚   â”œâ”€â”€ models/                # ãƒ‡ãƒ¼ã‚¿ãƒ¢ãƒ‡ãƒ«ãƒ»ã‚¹ã‚­ãƒ¼ãƒ
â”‚   â”‚   â”œâ”€â”€ __init__.py
â”‚   â”‚   â”œâ”€â”€ database.py        # SQLAlchemy ãƒ¢ãƒ‡ãƒ«
â”‚   â”‚   â”œâ”€â”€ schemas.py         # Pydantic ã‚¹ã‚­ãƒ¼ãƒ
â”‚   â”‚   â””â”€â”€ enums.py           # åˆ—æŒ™å‹å®šç¾©
â”‚   â”‚
â”‚   â”œâ”€â”€ sources/               # ãƒ‡ãƒ¼ã‚¿ã‚½ãƒ¼ã‚¹ç®¡ç†
â”‚   â”‚   â”œâ”€â”€ __init__.py
â”‚   â”‚   â”œâ”€â”€ source_manager.py  # ã‚½ãƒ¼ã‚¹çµ±åˆç®¡ç†
â”‚   â”‚   â”œâ”€â”€ pdf_source.py      # PDF ã‚½ãƒ¼ã‚¹å‡¦ç†
â”‚   â”‚   â”œâ”€â”€ web_source.py      # Web ã‚½ãƒ¼ã‚¹å‡¦ç†
â”‚   â”‚   â””â”€â”€ rss_source.py      # RSS ã‚½ãƒ¼ã‚¹å‡¦ç†
â”‚   â”‚
â”‚   â”œâ”€â”€ extractors/            # æ–‡æ›¸æŠ½å‡ºã‚¨ãƒ³ã‚¸ãƒ³
â”‚   â”‚   â”œâ”€â”€ __init__.py
â”‚   â”‚   â”œâ”€â”€ base_extractor.py  # æŠ½å‡ºå™¨åŸºåº•ã‚¯ãƒ©ã‚¹
â”‚   â”‚   â”œâ”€â”€ pdf_extractor.py   # PDF æŠ½å‡ºå™¨
â”‚   â”‚   â”œâ”€â”€ web_extractor.py   # Web æŠ½å‡ºå™¨
â”‚   â”‚   â””â”€â”€ content_detector.py # ã‚³ãƒ³ãƒ†ãƒ³ãƒ„ã‚¿ã‚¤ãƒ—æ¤œå‡º
â”‚   â”‚
â”‚   â”œâ”€â”€ processing/            # ãƒ†ã‚­ã‚¹ãƒˆå‡¦ç†ãƒ»ãƒãƒ£ãƒ³ã‚¯åŒ–
â”‚   â”‚   â”œâ”€â”€ __init__.py
â”‚   â”‚   â”œâ”€â”€ text_processor.py  # ãƒ†ã‚­ã‚¹ãƒˆå‰å‡¦ç†ãƒ»æ­£è¦åŒ–
â”‚   â”‚   â”œâ”€â”€ chunker.py         # ã‚»ãƒãƒ³ãƒ†ã‚£ãƒƒã‚¯ãƒãƒ£ãƒ³ã‚¯åˆ†å‰²
â”‚   â”‚   â”œâ”€â”€ language_detector.py # è¨€èªæ¤œå‡ºãƒ»å¤šè¨€èªå¯¾å¿œ
â”‚   â”‚   â””â”€â”€ batch_processor.py # ãƒãƒƒãƒå‡¦ç†ãƒ»é€²æ—ç®¡ç†
â”‚   â”‚
â”‚   â”œâ”€â”€ embeddings/            # åŸ‹ã‚è¾¼ã¿ç”Ÿæˆãƒ»ç®¡ç†
â”‚   â”‚   â”œâ”€â”€ __init__.py
â”‚   â”‚   â”œâ”€â”€ embedding_service.py # åŸ‹ã‚è¾¼ã¿ã‚µãƒ¼ãƒ“ã‚¹
â”‚   â”‚   â”œâ”€â”€ model_manager.py   # ãƒ¢ãƒ‡ãƒ«ç®¡ç†ãƒ»æœ€é©åŒ–
â”‚   â”‚   â””â”€â”€ batch_embedder.py  # ãƒãƒƒãƒåŸ‹ã‚è¾¼ã¿ç”Ÿæˆ
â”‚   â”‚
â”‚   â”œâ”€â”€ storage/               # ãƒ‡ãƒ¼ã‚¿ã‚¹ãƒˆãƒ¬ãƒ¼ã‚¸ç®¡ç†
â”‚   â”‚   â”œâ”€â”€ __init__.py
â”‚   â”‚   â”œâ”€â”€ vector_store.py    # Chroma DB ç®¡ç†
â”‚   â”‚   â”œâ”€â”€ metadata_store.py  # SQLite ãƒ¡ã‚¿ãƒ‡ãƒ¼ã‚¿ç®¡ç†
â”‚   â”‚   â””â”€â”€ file_store.py      # ãƒ•ã‚¡ã‚¤ãƒ«ã‚¹ãƒˆãƒ¬ãƒ¼ã‚¸ç®¡ç†
â”‚   â”‚
â”‚   â”œâ”€â”€ search/                # æ¤œç´¢ã‚¨ãƒ³ã‚¸ãƒ³ãƒ»ãƒ©ãƒ³ã‚­ãƒ³ã‚°
â”‚   â”‚   â”œâ”€â”€ __init__.py
â”‚   â”‚   â”œâ”€â”€ search_engine.py   # ã‚»ãƒãƒ³ãƒ†ã‚£ãƒƒã‚¯æ¤œç´¢ã‚¨ãƒ³ã‚¸ãƒ³
â”‚   â”‚   â”œâ”€â”€ ranking.py         # æ¤œç´¢çµæœãƒ©ãƒ³ã‚­ãƒ³ã‚°
â”‚   â”‚   â”œâ”€â”€ filters.py         # æ¤œç´¢ãƒ•ã‚£ãƒ«ã‚¿ãƒªãƒ³ã‚°
â”‚   â”‚   â””â”€â”€ analytics.py       # æ¤œç´¢åˆ†æãƒ»ãƒ­ã‚°
â”‚   â”‚
â”‚   â”œâ”€â”€ quality/               # å“è³ªç®¡ç†ãƒ»è©•ä¾¡
â”‚   â”‚   â”œâ”€â”€ __init__.py
â”‚   â”‚   â”œâ”€â”€ quality_evaluator.py # å“è³ªè©•ä¾¡ã‚·ã‚¹ãƒ†ãƒ 
â”‚   â”‚   â”œâ”€â”€ deduplication.py   # é‡è¤‡æ¤œå‡ºãƒ»é™¤å»
â”‚   â”‚   â””â”€â”€ quality_metrics.py # å“è³ªãƒ¡ãƒˆãƒªã‚¯ã‚¹è¨ˆç®—
â”‚   â”‚
â”‚   â”œâ”€â”€ api/                   # REST API ã‚¨ãƒ³ãƒ‰ãƒã‚¤ãƒ³ãƒˆ
â”‚   â”‚   â”œâ”€â”€ __init__.py
â”‚   â”‚   â”œâ”€â”€ search.py          # æ¤œç´¢API
â”‚   â”‚   â”œâ”€â”€ documents.py       # æ–‡æ›¸ç®¡ç†API
â”‚   â”‚   â”œâ”€â”€ sources.py         # ã‚½ãƒ¼ã‚¹ç®¡ç†API
â”‚   â”‚   â””â”€â”€ health.py          # ãƒ˜ãƒ«ã‚¹ãƒã‚§ãƒƒã‚¯API
â”‚   â”‚
â”‚   â”œâ”€â”€ ui/                    # Streamlit WebUI
â”‚   â”‚   â”œâ”€â”€ __init__.py
â”‚   â”‚   â”œâ”€â”€ streamlit_app.py   # ãƒ¡ã‚¤ãƒ³UIã‚¢ãƒ—ãƒªã‚±ãƒ¼ã‚·ãƒ§ãƒ³
â”‚   â”‚   â”œâ”€â”€ components/        # UI ã‚³ãƒ³ãƒãƒ¼ãƒãƒ³ãƒˆ
â”‚   â”‚   â”‚   â”œâ”€â”€ search_form.py # æ¤œç´¢ãƒ•ã‚©ãƒ¼ãƒ 
â”‚   â”‚   â”‚   â”œâ”€â”€ result_display.py # çµæœè¡¨ç¤º
â”‚   â”‚   â”‚   â”œâ”€â”€ source_viewer.py # ã‚½ãƒ¼ã‚¹è©³ç´°è¡¨ç¤º
â”‚   â”‚   â”‚   â””â”€â”€ user_feedback.py # ãƒ¦ãƒ¼ã‚¶ãƒ¼ãƒ•ã‚£ãƒ¼ãƒ‰ãƒãƒƒã‚¯
â”‚   â”‚   â””â”€â”€ styles/            # ã‚«ã‚¹ã‚¿ãƒ CSSãƒ»ã‚¹ã‚¿ã‚¤ãƒ«
â”‚   â”‚       â””â”€â”€ custom.css
â”‚   â”‚
â”‚   â””â”€â”€ admin/                 # ç®¡ç†æ©Ÿèƒ½ãƒ»ç›£è¦–
â”‚       â”œâ”€â”€ __init__.py
â”‚       â”œâ”€â”€ admin_dashboard.py # ç®¡ç†ãƒ€ãƒƒã‚·ãƒ¥ãƒœãƒ¼ãƒ‰
â”‚       â”œâ”€â”€ source_management.py # ã‚½ãƒ¼ã‚¹ç®¡ç†
â”‚       â””â”€â”€ system_monitor.py  # ã‚·ã‚¹ãƒ†ãƒ ç›£è¦–
â”‚
â”œâ”€â”€ tests/                     # ãƒ†ã‚¹ãƒˆã‚³ãƒ¼ãƒ‰
â”‚   â”œâ”€â”€ __init__.py
â”‚   â”œâ”€â”€ conftest.py           # pytestè¨­å®šãƒ»ãƒ•ã‚£ã‚¯ã‚¹ãƒãƒ£
â”‚   â”œâ”€â”€ unit/                 # å˜ä½“ãƒ†ã‚¹ãƒˆ
â”‚   â”‚   â”œâ”€â”€ test_extractors.py
â”‚   â”‚   â”œâ”€â”€ test_processing.py
â”‚   â”‚   â”œâ”€â”€ test_embeddings.py
â”‚   â”‚   â””â”€â”€ test_search.py
â”‚   â”œâ”€â”€ integration/          # çµ±åˆãƒ†ã‚¹ãƒˆ
â”‚   â”‚   â”œâ”€â”€ test_api.py
â”‚   â”‚   â”œâ”€â”€ test_database.py
â”‚   â”‚   â””â”€â”€ test_pipeline.py
â”‚   â””â”€â”€ e2e/                  # End-to-End ãƒ†ã‚¹ãƒˆ
â”‚       â””â”€â”€ test_user_scenarios.py
â”‚
â”œâ”€â”€ data/                     # ãƒ‡ãƒ¼ã‚¿ãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒª
â”‚   â”œâ”€â”€ sources/              # ã‚½ãƒ¼ã‚¹ãƒ•ã‚¡ã‚¤ãƒ«
â”‚   â”œâ”€â”€ processed/            # å‡¦ç†æ¸ˆã¿ãƒ•ã‚¡ã‚¤ãƒ«
â”‚   â”œâ”€â”€ embeddings/           # åŸ‹ã‚è¾¼ã¿ãƒ‡ãƒ¼ã‚¿
â”‚   â””â”€â”€ chroma_db/            # Chroma ãƒ‡ãƒ¼ã‚¿ãƒ™ãƒ¼ã‚¹
â”‚
â”œâ”€â”€ scripts/                  # é‹ç”¨ãƒ»ãƒ¡ãƒ³ãƒ†ãƒŠãƒ³ã‚¹ã‚¹ã‚¯ãƒªãƒ—ãƒˆ
â”‚   â”œâ”€â”€ setup_environment.py # ç’°å¢ƒåˆæœŸåŒ–
â”‚   â”œâ”€â”€ migrate_database.py  # ãƒ‡ãƒ¼ã‚¿ç§»è¡Œ
â”‚   â””â”€â”€ backup_data.py        # ãƒ‡ãƒ¼ã‚¿ãƒãƒƒã‚¯ã‚¢ãƒƒãƒ—
â”‚
â”œâ”€â”€ docs/                     # ãƒ‰ã‚­ãƒ¥ãƒ¡ãƒ³ãƒˆ
â”‚   â””â”€â”€ (æ—¢å­˜ã®å…¨ãƒ‰ã‚­ãƒ¥ãƒ¡ãƒ³ãƒˆ)
â”‚
â””â”€â”€ .github/                  # GitHub Actionsãƒ»CI/CD
    â””â”€â”€ workflows/
        â”œâ”€â”€ ci.yml            # ç¶™ç¶šçš„çµ±åˆ
        â””â”€â”€ release.yml       # ãƒªãƒªãƒ¼ã‚¹è‡ªå‹•åŒ–
```

## ğŸ”§ Core Servicesè©³ç´°ä»•æ§˜

### 1. Document Ingestion Service
```python
# erpfts/sources/source_manager.py
from typing import List, Dict, Any, Optional
from dataclasses import dataclass
from enum import Enum
import asyncio
from pathlib import Path

class SourceType(Enum):
    PDF = "pdf"
    WEB = "web"
    RSS = "rss"
    LOCAL_FILE = "local_file"

@dataclass
class SourceConfig:
    """ã‚½ãƒ¼ã‚¹è¨­å®šã‚¯ãƒ©ã‚¹"""
    id: str
    name: str
    source_type: SourceType
    base_url: Optional[str] = None
    file_path: Optional[Path] = None
    rss_feed: Optional[str] = None
    check_interval: int = 3600  # seconds
    is_active: bool = True
    quality_weight: float = 1.0
    metadata: Dict[str, Any] = None

class SourceManager:
    """çµ±åˆã‚½ãƒ¼ã‚¹ç®¡ç†ã‚·ã‚¹ãƒ†ãƒ """
    
    def __init__(self, db_session, file_store, quality_evaluator):
        self.db = db_session
        self.file_store = file_store
        self.quality_evaluator = quality_evaluator
        self.extractors = self._init_extractors()
    
    def _init_extractors(self):
        """æŠ½å‡ºå™¨ã®åˆæœŸåŒ–"""
        from erpfts.extractors import (
            PDFExtractor, WebExtractor, RSSExtractor
        )
        return {
            SourceType.PDF: PDFExtractor(),
            SourceType.WEB: WebExtractor(),
            SourceType.RSS: RSSExtractor(),
        }
    
    async def register_source(
        self, 
        source_config: SourceConfig
    ) -> Dict[str, Any]:
        """æ–°ã—ã„ã‚½ãƒ¼ã‚¹ã®ç™»éŒ²"""
        try:
            # 1. ã‚½ãƒ¼ã‚¹è¨­å®šã®æ¤œè¨¼
            await self._validate_source_config(source_config)
            
            # 2. ãƒ‡ãƒ¼ã‚¿ãƒ™ãƒ¼ã‚¹ã¸ã®ç™»éŒ²
            source_record = await self._create_source_record(source_config)
            
            # 3. åˆå›å–ã‚Šè¾¼ã¿ãƒ†ã‚¹ãƒˆ
            test_result = await self._test_source_ingestion(source_config)
            
            return {
                "status": "success",
                "source_id": source_record.id,
                "test_result": test_result
            }
        except Exception as e:
            return {
                "status": "error",
                "error": str(e)
            }
    
    async def ingest_source(
        self, 
        source_id: str,
        force_update: bool = False
    ) -> Dict[str, Any]:
        """ã‚½ãƒ¼ã‚¹ã‹ã‚‰ã®æ–‡æ›¸å–ã‚Šè¾¼ã¿å®Ÿè¡Œ"""
        # 1. ã‚½ãƒ¼ã‚¹è¨­å®šå–å¾—
        source_config = await self._get_source_config(source_id)
        
        # 2. æŠ½å‡ºå™¨é¸æŠãƒ»å®Ÿè¡Œ
        extractor = self.extractors[source_config.source_type]
        documents = await extractor.extract(source_config)
        
        # 3. å„æ–‡æ›¸ã®å‡¦ç†
        processed_docs = []
        for doc_data in documents:
            try:
                # é‡è¤‡ãƒã‚§ãƒƒã‚¯
                if not force_update and await self._is_duplicate(doc_data):
                    continue
                
                # æ–‡æ›¸å‡¦ç†ãƒ»å“è³ªè©•ä¾¡
                processed_doc = await self._process_document(
                    doc_data, source_config
                )
                processed_docs.append(processed_doc)
                
            except Exception as e:
                # ã‚¨ãƒ©ãƒ¼ãƒ­ã‚°è¨˜éŒ²ãƒ»ç¶™ç¶šå‡¦ç†
                await self._log_processing_error(doc_data, e)
        
        # 4. ãƒãƒƒãƒä¿å­˜ãƒ»çµ±è¨ˆæ›´æ–°
        result = await self._save_documents(processed_docs, source_id)
        
        return result

# Phase1 MVPç”¨ç°¡æ˜“å®Ÿè£…ä¾‹
class SimplePDFExtractor:
    """Phase1ç”¨ç°¡æ˜“PDFæŠ½å‡ºå™¨"""
    
    async def extract(self, source_config: SourceConfig) -> List[Dict]:
        """PDFæ–‡æ›¸ã‹ã‚‰ã®ãƒ†ã‚­ã‚¹ãƒˆæŠ½å‡º"""
        import PyPDF2
        from pathlib import Path
        
        if source_config.source_type != SourceType.PDF:
            raise ValueError("PDFã‚½ãƒ¼ã‚¹ä»¥å¤–ã¯å¯¾å¿œã—ã¦ã„ã¾ã›ã‚“")
        
        pdf_path = Path(source_config.file_path)
        if not pdf_path.exists():
            raise FileNotFoundError(f"PDF file not found: {pdf_path}")
        
        with open(pdf_path, 'rb') as file:
            pdf_reader = PyPDF2.PdfReader(file)
            text_content = ""
            
            for page_num, page in enumerate(pdf_reader.pages):
                page_text = page.extract_text()
                text_content += f"\n--- Page {page_num + 1} ---\n{page_text}"
        
        # åŸºæœ¬ãƒ¡ã‚¿ãƒ‡ãƒ¼ã‚¿ç”Ÿæˆ
        doc_metadata = {
            "title": pdf_path.stem,
            "source_type": "pdf",
            "file_path": str(pdf_path),
            "page_count": len(pdf_reader.pages),
            "content_hash": self._calculate_hash(text_content),
            "extraction_timestamp": datetime.utcnow().isoformat()
        }
        
        return [{
            "content": text_content,
            "metadata": doc_metadata
        }]
```

### 2. Text Processing Service
```python
# erpfts/processing/text_processor.py
import re
import unicodedata
from typing import List, Dict, Any
import spacy
from langdetect import detect
import tiktoken

class TextProcessor:
    """ãƒ†ã‚­ã‚¹ãƒˆå‰å‡¦ç†ãƒ»æ­£è¦åŒ–ã‚µãƒ¼ãƒ“ã‚¹"""
    
    def __init__(self):
        # spaCy å¤šè¨€èªãƒ¢ãƒ‡ãƒ«èª­ã¿è¾¼ã¿
        self.nlp_ja = spacy.load("ja_core_news_sm")
        self.nlp_en = spacy.load("en_core_web_sm")
        self.tokenizer = tiktoken.get_encoding("cl100k_base")
    
    async def process_text(
        self, 
        raw_text: str, 
        source_metadata: Dict[str, Any]
    ) -> Dict[str, Any]:
        """ãƒ†ã‚­ã‚¹ãƒˆã®åŒ…æ‹¬çš„å‰å‡¦ç†"""
        
        # 1. åŸºæœ¬ã‚¯ãƒªãƒ¼ãƒ‹ãƒ³ã‚°
        cleaned_text = self._clean_text(raw_text)
        
        # 2. è¨€èªæ¤œå‡º
        language = self._detect_language(cleaned_text)
        
        # 3. è¨€èªåˆ¥å‡¦ç†
        processed_text = await self._process_by_language(cleaned_text, language)
        
        # 4. å“è³ªè©•ä¾¡
        quality_score = self._evaluate_text_quality(processed_text)
        
        return {
            "processed_text": processed_text,
            "language": language,
            "quality_score": quality_score,
            "token_count": len(self.tokenizer.encode(processed_text)),
            "char_count": len(processed_text),
            "metadata": {
                **source_metadata,
                "processing_timestamp": datetime.utcnow().isoformat(),
                "language_detected": language
            }
        }
    
    def _clean_text(self, text: str) -> str:
        """åŸºæœ¬ãƒ†ã‚­ã‚¹ãƒˆã‚¯ãƒªãƒ¼ãƒ‹ãƒ³ã‚°"""
        # Unicodeæ­£è¦åŒ–
        text = unicodedata.normalize('NFKC', text)
        
        # åˆ¶å¾¡æ–‡å­—é™¤å»
        text = re.sub(r'[\x00-\x08\x0B\x0C\x0E-\x1F\x7F]', '', text)
        
        # é€£ç¶šç©ºç™½ãƒ»æ”¹è¡Œã®æ­£è¦åŒ–
        text = re.sub(r'\n\s*\n\s*\n', '\n\n', text)  # 3é€£ç¶šæ”¹è¡Œâ†’2æ”¹è¡Œ
        text = re.sub(r'[ \t]+', ' ', text)  # é€£ç¶šã‚¹ãƒšãƒ¼ã‚¹â†’å˜ä¸€ã‚¹ãƒšãƒ¼ã‚¹
        
        # æ–‡æ›¸æ§‹é€ ãƒãƒ¼ã‚«ãƒ¼é™¤å»ï¼ˆPDFã®ä¸è¦ãªãƒ˜ãƒƒãƒ€ãƒ¼ãƒ»ãƒ•ãƒƒã‚¿ãƒ¼ç­‰ï¼‰
        text = re.sub(r'-{3,}', '', text)  # ç½«ç·š
        text = re.sub(r'={3,}', '', text)  # ç­‰å·ç·š
        
        return text.strip()
    
    def _detect_language(self, text: str) -> str:
        """è¨€èªæ¤œå‡º"""
        try:
            # ãƒ†ã‚­ã‚¹ãƒˆã‚µãƒ³ãƒ—ãƒ«ä½¿ç”¨ï¼ˆæœ€åˆã®1000æ–‡å­—ï¼‰
            sample_text = text[:1000] if len(text) > 1000 else text
            detected = detect(sample_text)
            
            # ã‚µãƒãƒ¼ãƒˆè¨€èªã«ãƒãƒƒãƒ”ãƒ³ã‚°
            if detected in ['ja', 'japanese']:
                return 'ja'
            elif detected in ['en', 'english']:
                return 'en'
            else:
                return 'auto'  # ãã®ä»–è¨€èªã¯è‡ªå‹•åˆ¤å®š
        except:
            return 'auto'

# erpfts/processing/chunker.py
class SemanticChunker:
    """ã‚»ãƒãƒ³ãƒ†ã‚£ãƒƒã‚¯ãƒãƒ£ãƒ³ã‚¯åˆ†å‰²å™¨"""
    
    def __init__(self, max_tokens: int = 512, overlap_tokens: int = 50):
        self.max_tokens = max_tokens
        self.overlap_tokens = overlap_tokens
        self.tokenizer = tiktoken.get_encoding("cl100k_base")
    
    async def chunk_text(
        self, 
        text: str, 
        metadata: Dict[str, Any]
    ) -> List[Dict[str, Any]]:
        """ãƒ†ã‚­ã‚¹ãƒˆã®ã‚»ãƒãƒ³ãƒ†ã‚£ãƒƒã‚¯ãƒãƒ£ãƒ³ã‚¯åˆ†å‰²"""
        
        # 1. æ®µè½åˆ†å‰²
        paragraphs = self._split_into_paragraphs(text)
        
        # 2. ãƒˆãƒ¼ã‚¯ãƒ³æ•°ãƒ™ãƒ¼ã‚¹ã§ãƒãƒ£ãƒ³ã‚¯ä½œæˆ
        chunks = []
        current_chunk = ""
        current_tokens = 0
        
        for paragraph in paragraphs:
            paragraph_tokens = len(self.tokenizer.encode(paragraph))
            
            # æ®µè½ãŒmax_tokensã‚’è¶…ãˆã‚‹å ´åˆã¯åˆ†å‰²
            if paragraph_tokens > self.max_tokens:
                # ç¾åœ¨ã®ãƒãƒ£ãƒ³ã‚¯ã‚’ä¿å­˜
                if current_chunk:
                    chunks.append(self._create_chunk_dict(
                        current_chunk, len(chunks), metadata
                    ))
                    current_chunk = ""
                    current_tokens = 0
                
                # å¤§ããªæ®µè½ã‚’åˆ†å‰²
                split_chunks = self._split_large_paragraph(
                    paragraph, len(chunks), metadata
                )
                chunks.extend(split_chunks)
            
            # é€šå¸¸ã®æ®µè½è¿½åŠ å‡¦ç†
            elif current_tokens + paragraph_tokens <= self.max_tokens:
                current_chunk += f"\n\n{paragraph}" if current_chunk else paragraph
                current_tokens += paragraph_tokens
            else:
                # ç¾åœ¨ã®ãƒãƒ£ãƒ³ã‚¯ã‚’ä¿å­˜ãƒ»æ–°ãƒãƒ£ãƒ³ã‚¯é–‹å§‹
                chunks.append(self._create_chunk_dict(
                    current_chunk, len(chunks), metadata
                ))
                current_chunk = paragraph
                current_tokens = paragraph_tokens
        
        # æœ€å¾Œã®ãƒãƒ£ãƒ³ã‚¯å‡¦ç†
        if current_chunk:
            chunks.append(self._create_chunk_dict(
                current_chunk, len(chunks), metadata
            ))
        
        return chunks
    
    def _create_chunk_dict(
        self, 
        content: str, 
        chunk_index: int, 
        metadata: Dict[str, Any]
    ) -> Dict[str, Any]:
        """ãƒãƒ£ãƒ³ã‚¯è¾æ›¸ä½œæˆ"""
        token_count = len(self.tokenizer.encode(content))
        
        return {
            "content": content.strip(),
            "chunk_index": chunk_index,
            "token_count": token_count,
            "char_count": len(content),
            "content_hash": hashlib.md5(content.encode()).hexdigest(),
            "metadata": {
                **metadata,
                "chunk_timestamp": datetime.utcnow().isoformat()
            }
        }
```

### 3. Embedding Service
```python
# erpfts/embeddings/embedding_service.py
from sentence_transformers import SentenceTransformer
import numpy as np
from typing import List, Dict, Any, Optional
import asyncio
import torch

class EmbeddingService:
    """åŸ‹ã‚è¾¼ã¿ç”Ÿæˆãƒ»ç®¡ç†ã‚µãƒ¼ãƒ“ã‚¹"""
    
    def __init__(self, model_name: str = "intfloat/multilingual-e5-large"):
        self.model_name = model_name
        self.model = None
        self.device = "cuda" if torch.cuda.is_available() else "cpu"
        self._load_model()
    
    def _load_model(self):
        """åŸ‹ã‚è¾¼ã¿ãƒ¢ãƒ‡ãƒ«èª­ã¿è¾¼ã¿"""
        try:
            self.model = SentenceTransformer(self.model_name)
            self.model.to(self.device)
            print(f"âœ… Embedding model loaded: {self.model_name} on {self.device}")
        except Exception as e:
            raise RuntimeError(f"Failed to load embedding model: {e}")
    
    async def generate_embeddings(
        self, 
        texts: List[str],
        batch_size: int = 32
    ) -> List[np.ndarray]:
        """ãƒ†ã‚­ã‚¹ãƒˆãƒªã‚¹ãƒˆã®åŸ‹ã‚è¾¼ã¿ç”Ÿæˆ"""
        if not texts:
            return []
        
        try:
            # ãƒãƒƒãƒå‡¦ç†ã§åŸ‹ã‚è¾¼ã¿ç”Ÿæˆ
            embeddings = []
            for i in range(0, len(texts), batch_size):
                batch = texts[i:i + batch_size]
                
                # å¤šè¨€èªE5ãƒ¢ãƒ‡ãƒ«ç”¨ã®ãƒ—ãƒ¬ãƒ•ã‚£ãƒƒã‚¯ã‚¹è¿½åŠ 
                prefixed_batch = [f"query: {text}" for text in batch]
                
                # åŸ‹ã‚è¾¼ã¿ç”Ÿæˆ
                batch_embeddings = self.model.encode(
                    prefixed_batch,
                    convert_to_numpy=True,
                    normalize_embeddings=True,
                    show_progress_bar=True
                )
                embeddings.extend(batch_embeddings)
                
                # éåŒæœŸyieldï¼ˆUIã®å¿œç­”æ€§å‘ä¸Šï¼‰
                await asyncio.sleep(0)
            
            return embeddings
            
        except Exception as e:
            raise RuntimeError(f"Embedding generation failed: {e}")
    
    async def generate_single_embedding(self, text: str) -> np.ndarray:
        """å˜ä¸€ãƒ†ã‚­ã‚¹ãƒˆã®åŸ‹ã‚è¾¼ã¿ç”Ÿæˆ"""
        embeddings = await self.generate_embeddings([text])
        return embeddings[0] if embeddings else None
    
    def get_embedding_dimension(self) -> int:
        """åŸ‹ã‚è¾¼ã¿æ¬¡å…ƒæ•°å–å¾—"""
        return self.model.get_sentence_embedding_dimension()
    
    async def similarity_search(
        self, 
        query_text: str,
        candidate_embeddings: List[np.ndarray],
        top_k: int = 10
    ) -> List[Dict[str, Any]]:
        """é¡ä¼¼åº¦æ¤œç´¢å®Ÿè¡Œ"""
        # ã‚¯ã‚¨ãƒªåŸ‹ã‚è¾¼ã¿ç”Ÿæˆ
        query_embedding = await self.generate_single_embedding(query_text)
        
        # ã‚³ã‚µã‚¤ãƒ³é¡ä¼¼åº¦è¨ˆç®—
        similarities = []
        for idx, candidate in enumerate(candidate_embeddings):
            similarity = np.dot(query_embedding, candidate) / (
                np.linalg.norm(query_embedding) * np.linalg.norm(candidate)
            )
            similarities.append({
                "index": idx,
                "similarity": float(similarity)
            })
        
        # é¡ä¼¼åº¦ã§ã‚½ãƒ¼ãƒˆãƒ»ä¸Šä½Kä»¶å–å¾—
        similarities.sort(key=lambda x: x["similarity"], reverse=True)
        return similarities[:top_k]

# Phase1ç”¨Chromaçµ±åˆä¾‹
class ChromaVectorStore:
    """Chroma ãƒ™ã‚¯ãƒˆãƒ«ã‚¹ãƒˆã‚¢ç®¡ç†"""
    
    def __init__(self, collection_name: str = "erpfts_embeddings"):
        import chromadb
        self.client = chromadb.PersistentClient(path="./data/chroma_db")
        self.collection_name = collection_name
        self.collection = self._get_or_create_collection()
    
    def _get_or_create_collection(self):
        """ã‚³ãƒ¬ã‚¯ã‚·ãƒ§ãƒ³å–å¾—ã¾ãŸã¯ä½œæˆ"""
        try:
            return self.client.get_collection(self.collection_name)
        except:
            return self.client.create_collection(
                name=self.collection_name,
                metadata={"description": "ERP Knowledge Base Embeddings"}
            )
    
    async def add_embeddings(
        self, 
        embeddings: List[np.ndarray],
        texts: List[str],
        metadatas: List[Dict[str, Any]],
        ids: List[str]
    ):
        """åŸ‹ã‚è¾¼ã¿ã‚’Chromaã«è¿½åŠ """
        try:
            self.collection.add(
                embeddings=[emb.tolist() for emb in embeddings],
                documents=texts,
                metadatas=metadatas,
                ids=ids
            )
        except Exception as e:
            raise RuntimeError(f"Failed to add embeddings to Chroma: {e}")
    
    async def search(
        self, 
        query_embedding: np.ndarray,
        n_results: int = 10,
        where: Optional[Dict[str, Any]] = None
    ) -> Dict[str, Any]:
        """ãƒ™ã‚¯ãƒˆãƒ«æ¤œç´¢å®Ÿè¡Œ"""
        try:
            results = self.collection.query(
                query_embeddings=[query_embedding.tolist()],
                n_results=n_results,
                where=where,
                include=["documents", "metadatas", "distances"]
            )
            return results
        except Exception as e:
            raise RuntimeError(f"Vector search failed: {e}")
```

### 4. Search Engine
```python
# erpfts/search/search_engine.py
from typing import List, Dict, Any, Optional
from dataclasses import dataclass
import asyncio

@dataclass
class SearchQuery:
    """æ¤œç´¢ã‚¯ã‚¨ãƒªã‚¯ãƒ©ã‚¹"""
    query_text: str
    filters: Optional[Dict[str, Any]] = None
    limit: int = 10
    offset: int = 0
    min_similarity: float = 0.0

@dataclass 
class SearchResult:
    """æ¤œç´¢çµæœã‚¯ãƒ©ã‚¹"""
    chunk_id: str
    document_id: str
    content: str
    similarity_score: float
    metadata: Dict[str, Any]
    source_info: Dict[str, Any]

class SearchEngine:
    """ã‚»ãƒãƒ³ãƒ†ã‚£ãƒƒã‚¯æ¤œç´¢ã‚¨ãƒ³ã‚¸ãƒ³"""
    
    def __init__(
        self, 
        embedding_service: EmbeddingService,
        vector_store: ChromaVectorStore,
        metadata_store: MetadataStore
    ):
        self.embedding_service = embedding_service
        self.vector_store = vector_store
        self.metadata_store = metadata_store
    
    async def search(self, search_query: SearchQuery) -> List[SearchResult]:
        """ã‚»ãƒãƒ³ãƒ†ã‚£ãƒƒã‚¯æ¤œç´¢å®Ÿè¡Œ"""
        try:
            # 1. ã‚¯ã‚¨ãƒªåŸ‹ã‚è¾¼ã¿ç”Ÿæˆ
            query_embedding = await self.embedding_service.generate_single_embedding(
                search_query.query_text
            )
            
            # 2. ãƒ™ã‚¯ãƒˆãƒ«æ¤œç´¢å®Ÿè¡Œ
            vector_results = await self.vector_store.search(
                query_embedding=query_embedding,
                n_results=search_query.limit * 2,  # ä½™è£•ã‚’æŒã£ã¦å–å¾—
                where=self._build_chroma_filters(search_query.filters)
            )
            
            # 3. ãƒ¡ã‚¿ãƒ‡ãƒ¼ã‚¿æ‹¡å……ãƒ»çµæœæ§‹é€ åŒ–
            search_results = []
            for i, chunk_id in enumerate(vector_results["ids"][0]):
                similarity = 1.0 - vector_results["distances"][0][i]  # distance -> similarity
                
                if similarity < search_query.min_similarity:
                    continue
                
                # ãƒ¡ã‚¿ãƒ‡ãƒ¼ã‚¿å–å¾—ãƒ»æ‹¡å……
                chunk_metadata = vector_results["metadatas"][0][i]
                document_info = await self.metadata_store.get_document_info(
                    chunk_metadata["document_id"]
                )
                source_info = await self.metadata_store.get_source_info(
                    document_info["source_id"]
                )
                
                search_result = SearchResult(
                    chunk_id=chunk_id,
                    document_id=chunk_metadata["document_id"],
                    content=vector_results["documents"][0][i],
                    similarity_score=similarity,
                    metadata=chunk_metadata,
                    source_info=source_info
                )
                search_results.append(search_result)
            
            # 4. çµæœãƒ©ãƒ³ã‚­ãƒ³ã‚°ãƒ»ãƒ•ã‚£ãƒ«ã‚¿ãƒªãƒ³ã‚°
            ranked_results = await self._rank_results(
                search_results, search_query
            )
            
            # 5. ãƒšãƒ¼ã‚¸ãƒãƒ¼ã‚·ãƒ§ãƒ³é©ç”¨
            start_idx = search_query.offset
            end_idx = start_idx + search_query.limit
            
            return ranked_results[start_idx:end_idx]
            
        except Exception as e:
            raise RuntimeError(f"Search execution failed: {e}")
    
    async def _rank_results(
        self, 
        results: List[SearchResult], 
        query: SearchQuery
    ) -> List[SearchResult]:
        """æ¤œç´¢çµæœãƒ©ãƒ³ã‚­ãƒ³ã‚°"""
        # Phase1ã§ã¯å˜ç´”ãªé¡ä¼¼åº¦ã‚½ãƒ¼ãƒˆ
        # Phase2ä»¥é™ã§é«˜åº¦ãƒ©ãƒ³ã‚­ãƒ³ã‚°ï¼ˆå“è³ªã‚¹ã‚³ã‚¢ã€äººæ°—åº¦ã€æ™‚é–“ç­‰ï¼‰ã‚’è¿½åŠ 
        
        def ranking_score(result: SearchResult) -> float:
            base_score = result.similarity_score
            
            # å“è³ªã‚¹ã‚³ã‚¢é‡ã¿ä»˜ã‘ï¼ˆ30%ï¼‰
            quality_score = result.metadata.get("quality_score", 3.0) / 5.0
            quality_weight = base_score * 0.3 * quality_score
            
            # ã‚½ãƒ¼ã‚¹å“è³ªé‡ã¿ä»˜ã‘ï¼ˆ20%ï¼‰
            source_weight = result.source_info.get("quality_weight", 1.0)
            source_score = base_score * 0.2 * source_weight
            
            return base_score + quality_weight + source_score
        
        results.sort(key=ranking_score, reverse=True)
        return results
    
    def _build_chroma_filters(self, filters: Optional[Dict[str, Any]]) -> Optional[Dict]:
        """Chromaç”¨ãƒ•ã‚£ãƒ«ã‚¿ãƒ¼æ§‹ç¯‰"""
        if not filters:
            return None
        
        chroma_filter = {}
        
        # ã‚½ãƒ¼ã‚¹ã‚¿ã‚¤ãƒ—ãƒ•ã‚£ãƒ«ã‚¿ãƒ¼
        if "source_types" in filters:
            chroma_filter["source_type"] = {"$in": filters["source_types"]}
        
        # å“è³ªã‚¹ã‚³ã‚¢ãƒ•ã‚£ãƒ«ã‚¿ãƒ¼
        if "min_quality_score" in filters:
            chroma_filter["quality_score"] = {"$gte": filters["min_quality_score"]}
        
        # è¨€èªãƒ•ã‚£ãƒ«ã‚¿ãƒ¼
        if "languages" in filters:
            chroma_filter["language"] = {"$in": filters["languages"]}
        
        return chroma_filter if chroma_filter else None

# æ¤œç´¢APIå®Ÿè£…ä¾‹
# erpfts/api/search.py
from fastapi import APIRouter, HTTPException, Depends
from pydantic import BaseModel
from typing import List, Optional, Dict, Any

router = APIRouter(prefix="/api/v1/search", tags=["search"])

class SearchRequest(BaseModel):
    query: str
    filters: Optional[Dict[str, Any]] = None
    limit: int = 10
    offset: int = 0
    min_similarity: float = 0.0

class SearchResponse(BaseModel):
    results: List[Dict[str, Any]]
    total_count: int
    query_time: float
    
@router.post("/", response_model=SearchResponse)
async def search_documents(
    request: SearchRequest,
    search_engine: SearchEngine = Depends(get_search_engine)
):
    """æ–‡æ›¸æ¤œç´¢API"""
    try:
        import time
        start_time = time.time()
        
        # æ¤œç´¢å®Ÿè¡Œ
        search_query = SearchQuery(
            query_text=request.query,
            filters=request.filters,
            limit=request.limit,
            offset=request.offset,
            min_similarity=request.min_similarity
        )
        
        results = await search_engine.search(search_query)
        query_time = time.time() - start_time
        
        # çµæœæ§‹é€ åŒ–
        formatted_results = [
            {
                "chunk_id": result.chunk_id,
                "document_id": result.document_id,
                "content": result.content,
                "similarity_score": result.similarity_score,
                "metadata": result.metadata,
                "source_info": result.source_info
            }
            for result in results
        ]
        
        return SearchResponse(
            results=formatted_results,
            total_count=len(formatted_results),
            query_time=query_time
        )
        
    except Exception as e:
        raise HTTPException(status_code=500, detail=str(e))
```

### 5. Streamlit WebUI
```python
# erpfts/ui/streamlit_app.py
import streamlit as st
import asyncio
import requests
from typing import List, Dict, Any
import time

# ãƒšãƒ¼ã‚¸è¨­å®š
st.set_page_config(
    page_title="ERPçŸ¥è­˜RAGã‚·ã‚¹ãƒ†ãƒ ",
    page_icon="ğŸ”",
    layout="wide",
    initial_sidebar_state="expanded"
)

# ã‚«ã‚¹ã‚¿ãƒ CSSèª­ã¿è¾¼ã¿
def load_css():
    """ã‚«ã‚¹ã‚¿ãƒ CSSèª­ã¿è¾¼ã¿"""
    with open("erpfts/ui/styles/custom.css") as f:
        st.markdown(f"<style>{f.read()}</style>", unsafe_allow_html=True)

def main():
    """ãƒ¡ã‚¤ãƒ³ã‚¢ãƒ—ãƒªã‚±ãƒ¼ã‚·ãƒ§ãƒ³"""
    load_css()
    
    # ã‚µã‚¤ãƒ‰ãƒãƒ¼è¨­å®š
    with st.sidebar:
        st.title("ğŸ” ERPçŸ¥è­˜æ¤œç´¢")
        st.markdown("---")
        
        # æ¤œç´¢ãƒ•ã‚£ãƒ«ã‚¿ãƒ¼
        st.subheader("æ¤œç´¢ãƒ•ã‚£ãƒ«ã‚¿ãƒ¼")
        source_types = st.multiselect(
            "ã‚½ãƒ¼ã‚¹ã‚¿ã‚¤ãƒ—",
            ["pdf", "web", "rss"],
            default=["pdf", "web", "rss"]
        )
        
        min_quality = st.slider(
            "æœ€å°å“è³ªã‚¹ã‚³ã‚¢",
            min_value=0.0,
            max_value=5.0,
            value=3.0,
            step=0.5
        )
        
        languages = st.multiselect(
            "è¨€èª",
            ["ja", "en"],
            default=["ja", "en"]
        )
    
    # ãƒ¡ã‚¤ãƒ³ã‚³ãƒ³ãƒ†ãƒ³ãƒ„ã‚¨ãƒªã‚¢
    st.title("ğŸ¢ ERPçŸ¥è­˜RAGã‚·ã‚¹ãƒ†ãƒ ")
    st.markdown("çµ„ç¹”ã®çŸ¥è­˜ã‚’åŠ¹ç‡çš„ã«æ¤œç´¢ãƒ»ç™ºè¦‹ã™ã‚‹ãŸã‚ã®ã‚·ã‚¹ãƒ†ãƒ ã§ã™")
    
    # æ¤œç´¢ãƒ•ã‚©ãƒ¼ãƒ 
    search_query = st.text_input(
        "æ¤œç´¢ã‚¯ã‚¨ãƒªã‚’å…¥åŠ›ã—ã¦ãã ã•ã„",
        placeholder="ä¾‹: ãƒ—ãƒ­ã‚¸ã‚§ã‚¯ãƒˆç®¡ç†ã®ãƒ™ã‚¹ãƒˆãƒ—ãƒ©ã‚¯ãƒ†ã‚£ã‚¹",
        help="è‡ªç„¶è¨€èªã§ã®æ¤œç´¢ãŒå¯èƒ½ã§ã™"
    )
    
    col1, col2, col3 = st.columns([1, 1, 2])
    with col1:
        search_button = st.button("ğŸ” æ¤œç´¢å®Ÿè¡Œ", type="primary")
    with col2:
        limit = st.selectbox("è¡¨ç¤ºä»¶æ•°", [5, 10, 20, 50], index=1)
    
    # æ¤œç´¢å®Ÿè¡Œ
    if search_button and search_query:
        with st.spinner("æ¤œç´¢ä¸­..."):
            results = search_documents(
                query=search_query,
                filters={
                    "source_types": source_types,
                    "min_quality_score": min_quality,
                    "languages": languages
                },
                limit=limit
            )
        
        # çµæœè¡¨ç¤º
        if results and results.get("results"):
            display_search_results(results)
        else:
            st.warning("æ¤œç´¢çµæœãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“ã§ã—ãŸã€‚åˆ¥ã®ã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰ã§è©¦ã—ã¦ãã ã•ã„ã€‚")
    
    # æœ€è¿‘ã®æ¤œç´¢å±¥æ­´ï¼ˆã‚»ãƒƒã‚·ãƒ§ãƒ³çŠ¶æ…‹ä½¿ç”¨ï¼‰
    if "search_history" not in st.session_state:
        st.session_state.search_history = []
    
    if st.session_state.search_history:
        st.markdown("---")
        st.subheader("ğŸ“‹ æœ€è¿‘ã®æ¤œç´¢")
        for i, hist_query in enumerate(st.session_state.search_history[-5:]):
            if st.button(f"ğŸ”„ {hist_query}", key=f"hist_{i}"):
                st.rerun()

def search_documents(
    query: str, 
    filters: Dict[str, Any],
    limit: int = 10
) -> Dict[str, Any]:
    """æ¤œç´¢APIå‘¼ã³å‡ºã—"""
    try:
        # FastAPI ã‚¨ãƒ³ãƒ‰ãƒã‚¤ãƒ³ãƒˆå‘¼ã³å‡ºã—
        response = requests.post(
            "http://localhost:8000/api/v1/search/",
            json={
                "query": query,
                "filters": filters,
                "limit": limit
            },
            timeout=30
        )
        response.raise_for_status()
        
        # æ¤œç´¢å±¥æ­´æ›´æ–°
        if query not in st.session_state.search_history:
            st.session_state.search_history.append(query)
        
        return response.json()
        
    except requests.RequestException as e:
        st.error(f"æ¤œç´¢APIã‚¨ãƒ©ãƒ¼: {str(e)}")
        return {}

def display_search_results(results: Dict[str, Any]):
    """æ¤œç´¢çµæœè¡¨ç¤º"""
    search_results = results.get("results", [])
    query_time = results.get("query_time", 0)
    
    # æ¤œç´¢ã‚µãƒãƒªãƒ¼
    st.success(f"ğŸ¯ {len(search_results)}ä»¶ã®çµæœãŒè¦‹ã¤ã‹ã‚Šã¾ã—ãŸï¼ˆ{query_time:.2f}ç§’ï¼‰")
    
    # çµæœãƒªã‚¹ãƒˆ
    for i, result in enumerate(search_results):
        with st.expander(
            f"ğŸ“„ {result['metadata'].get('title', 'ç„¡é¡Œ')} "
            f"(é¡ä¼¼åº¦: {result['similarity_score']:.1%})",
            expanded=(i < 3)  # æœ€åˆã®3ä»¶ã¯å±•é–‹
        ):
            # åŸºæœ¬æƒ…å ±
            col1, col2, col3 = st.columns(3)
            with col1:
                st.metric("é¡ä¼¼åº¦", f"{result['similarity_score']:.1%}")
            with col2:
                st.metric("å“è³ªã‚¹ã‚³ã‚¢", f"{result['metadata'].get('quality_score', 0):.1f}/5.0")
            with col3:
                st.metric("ã‚½ãƒ¼ã‚¹", result['source_info'].get('name', 'ä¸æ˜'))
            
            # ã‚³ãƒ³ãƒ†ãƒ³ãƒ„è¡¨ç¤º
            st.markdown("**ğŸ“ å†…å®¹:**")
            content_preview = result['content'][:500] + "..." if len(result['content']) > 500 else result['content']
            st.markdown(f"```\n{content_preview}\n```")
            
            # ãƒ¡ã‚¿ãƒ‡ãƒ¼ã‚¿
            with st.expander("ğŸ” è©³ç´°æƒ…å ±"):
                st.json({
                    "document_id": result['document_id'],
                    "chunk_id": result['chunk_id'],
                    "metadata": result['metadata'],
                    "source_info": result['source_info']
                })
            
            # ãƒ¦ãƒ¼ã‚¶ãƒ¼ãƒ•ã‚£ãƒ¼ãƒ‰ãƒãƒƒã‚¯
            col1, col2 = st.columns(2)
            with col1:
                if st.button(f"ğŸ‘ å½¹ã«ç«‹ã£ãŸ", key=f"helpful_{i}"):
                    record_feedback(result['chunk_id'], "helpful")
                    st.success("ãƒ•ã‚£ãƒ¼ãƒ‰ãƒãƒƒã‚¯ã‚ã‚ŠãŒã¨ã†ã”ã–ã„ã¾ã™ï¼")
            with col2:
                if st.button(f"ğŸ‘ å½¹ã«ç«‹ãŸãªã‹ã£ãŸ", key=f"not_helpful_{i}"):
                    record_feedback(result['chunk_id'], "not_helpful")
                    st.success("ãƒ•ã‚£ãƒ¼ãƒ‰ãƒãƒƒã‚¯ã‚ã‚ŠãŒã¨ã†ã”ã–ã„ã¾ã™ï¼")

def record_feedback(chunk_id: str, feedback_type: str):
    """ãƒ¦ãƒ¼ã‚¶ãƒ¼ãƒ•ã‚£ãƒ¼ãƒ‰ãƒãƒƒã‚¯è¨˜éŒ²"""
    try:
        requests.post(
            "http://localhost:8000/api/v1/feedback/",
            json={
                "chunk_id": chunk_id,
                "feedback_type": feedback_type,
                "timestamp": time.time()
            }
        )
    except:
        pass  # ãƒ•ã‚£ãƒ¼ãƒ‰ãƒãƒƒã‚¯å¤±æ•—ã¯ç„¡è¦–

if __name__ == "__main__":
    main()
```

## ğŸ¤– Implementation Notes for AI

### Critical Implementation Sequence
1. **Week 1**: åŸºç›¤ãƒ»è¨­å®šã‚·ã‚¹ãƒ†ãƒ  â†’ ãƒ‡ãƒ¼ã‚¿ãƒ™ãƒ¼ã‚¹ã‚¹ã‚­ãƒ¼ãƒ â†’ é–‹ç™ºç’°å¢ƒçµ±ä¸€
2. **Week 2**: ã‚½ãƒ¼ã‚¹ç®¡ç† â†’ æ–‡æ›¸æŠ½å‡º â†’ ãƒ†ã‚­ã‚¹ãƒˆå‡¦ç† â†’ ãƒãƒ£ãƒ³ã‚¯åˆ†å‰²
3. **Week 3**: åŸ‹ã‚è¾¼ã¿ç”Ÿæˆ â†’ ãƒ™ã‚¯ãƒˆãƒ«ã‚¹ãƒˆã‚¢ â†’ æ¤œç´¢ã‚¨ãƒ³ã‚¸ãƒ³
4. **Week 4**: FastAPI â†’ Streamlit UI â†’ çµ±åˆãƒ†ã‚¹ãƒˆ
5. **Week 5-6**: å“è³ªå‘ä¸Š â†’ ç®¡ç†æ©Ÿèƒ½ â†’ ãƒ‡ãƒ—ãƒ­ã‚¤æº–å‚™

### Key Dependencies & Integration Points
- **Chroma DB**: æ°¸ç¶šåŒ–è¨­å®šãƒ»ã‚³ãƒ¬ã‚¯ã‚·ãƒ§ãƒ³ç®¡ç†ãƒ»ãƒ¡ã‚¿ãƒ‡ãƒ¼ã‚¿çµ±åˆ
- **Multilingual-e5-large**: ãƒ¢ãƒ‡ãƒ«èª­ã¿è¾¼ã¿ãƒ»GPUå¯¾å¿œãƒ»ãƒãƒƒãƒå‡¦ç†
- **FastAPI + Streamlit**: éåŒæœŸå‡¦ç†ãƒ»ã‚¨ãƒ©ãƒ¼ãƒãƒ³ãƒ‰ãƒªãƒ³ã‚°ãƒ»CORSè¨­å®š
- **SQLAlchemy 2.0**: éåŒæœŸORMãƒ»ãƒã‚¤ã‚°ãƒ¬ãƒ¼ã‚·ãƒ§ãƒ³ãƒ»é–¢ä¿‚å®šç¾©

### Performance Considerations
- **åŸ‹ã‚è¾¼ã¿ç”Ÿæˆ**: ãƒãƒƒãƒã‚µã‚¤ã‚º32ã€GPUä½¿ç”¨æ™‚ã¯64ã«èª¿æ•´
- **æ¤œç´¢ãƒ¬ã‚¹ãƒãƒ³ã‚¹**: 3ç§’ä»¥å†…é”æˆã€å¿…è¦ã«å¿œã˜ã¦ã‚­ãƒ£ãƒƒã‚·ãƒ¥è¿½åŠ 
- **UIå¿œç­”æ€§**: StreamlitéåŒæœŸå‡¦ç†ã€ãƒ—ãƒ­ã‚°ãƒ¬ã‚¹ãƒãƒ¼è¡¨ç¤º
- **ãƒ¡ãƒ¢ãƒªä½¿ç”¨é‡**: å¤§é‡æ–‡æ›¸å‡¦ç†æ™‚ã®ãƒãƒƒãƒã‚µã‚¤ã‚ºèª¿æ•´

### Quality Gates
- **Week 2**: æ–‡æ›¸å–ã‚Šè¾¼ã¿æˆåŠŸç‡90%ã€å‡¦ç†æ™‚é–“10æ–‡æ›¸/åˆ†
- **Week 3**: æ¤œç´¢ç²¾åº¦70%ä»¥ä¸Šã€ãƒ¬ã‚¹ãƒãƒ³ã‚¹3ç§’ä»¥å†…
- **Week 4**: UIæ“ä½œæˆåŠŸç‡95%ã€ã‚¨ãƒ©ãƒ¼ãƒãƒ³ãƒ‰ãƒªãƒ³ã‚°å®Œå‚™
- **Week 6**: çµ±åˆãƒ†ã‚¹ãƒˆåˆæ ¼ã€æœ¬ç•ªãƒ‡ãƒ—ãƒ­ã‚¤æˆåŠŸ

---

**Version**: 1.0.0 | **Last Updated**: 2025-01-21 | **Next Review**: Weekly Development Review