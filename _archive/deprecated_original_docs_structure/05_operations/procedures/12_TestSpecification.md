# ERPçŸ¥è­˜RAGã‚·ã‚¹ãƒ†ãƒ  - ãƒ†ã‚¹ãƒˆä»•æ§˜æ›¸

---
doc_type: "test_specification"
complexity: "high"
estimated_effort: "60-80 hours"
prerequisites: ["02_SystemArchitecture.md", "03_FunctionalRequirements.md", "04_NonFunctionalRequirements.md", "05_DataModelDesign.md", "06_APISpecification.md", "11_SecurityDesign.md"]
implementation_priority: "medium"
ai_assistance_level: "full_automation_possible"
version: "1.0.0"
author: "Claude Code"
created_date: "2025-01-21"
status: "approved"
approval_authority: "Project Stakeholders"
---

## ğŸ“‹ ãƒ†ã‚¹ãƒˆä»•æ§˜æ¦‚è¦

### ãƒ†ã‚¹ãƒˆæˆ¦ç•¥ã®ç›®çš„
æœ¬æ–‡æ›¸ã¯ã€ŒERPçŸ¥è­˜RAGã‚·ã‚¹ãƒ†ãƒ ï¼ˆERPFTSï¼‰ã€ã«ãŠã‘ã‚‹åŒ…æ‹¬çš„ãªãƒ†ã‚¹ãƒˆæˆ¦ç•¥ã€ãƒ†ã‚¹ãƒˆã‚±ãƒ¼ã‚¹ã€å“è³ªä¿è¨¼ãƒ—ãƒ­ã‚»ã‚¹ã‚’å®šç¾©ã™ã‚‹ã€‚ã‚·ã‚¹ãƒ†ãƒ ã®æ©Ÿèƒ½æ€§ã€æ€§èƒ½ã€ã‚»ã‚­ãƒ¥ãƒªãƒ†ã‚£ã€ä¿¡é ¼æ€§ã‚’å¤šè§’çš„ã«æ¤œè¨¼ã—ã€é«˜å“è³ªãªã‚·ã‚¹ãƒ†ãƒ æä¾›ã‚’å®Ÿç¾ã™ã‚‹ã€‚

### ãƒ†ã‚¹ãƒˆåŸºæœ¬æ–¹é‡
```yaml
å“è³ªä¿è¨¼åŸå‰‡:
  æ—©æœŸç™ºè¦‹: ãƒ†ã‚¹ãƒˆé§†å‹•é–‹ç™ºã«ã‚ˆã‚‹æ—©æœŸå“è³ªç¢ºä¿
  ç¶™ç¶šçš„ãƒ†ã‚¹ãƒˆ: CI/CDãƒ‘ã‚¤ãƒ—ãƒ©ã‚¤ãƒ³ã§ã®è‡ªå‹•ãƒ†ã‚¹ãƒˆå®Ÿè¡Œ
  å¤šå±¤ãƒ†ã‚¹ãƒˆ: å˜ä½“ãƒ»çµ±åˆãƒ»ã‚·ã‚¹ãƒ†ãƒ ãƒ»å—å…¥ãƒ†ã‚¹ãƒˆã®çµ„ã¿åˆã‚ã›
  ãƒªã‚¹ã‚¯ãƒ™ãƒ¼ã‚¹: é‡è¦åº¦ãƒ»å½±éŸ¿åº¦ã«åŸºã¥ãå„ªå…ˆåº¦ä»˜ã‘

ãƒ†ã‚¹ãƒˆå¯¾è±¡ç¯„å›²:
  - æ©Ÿèƒ½ãƒ†ã‚¹ãƒˆ: å…¨æ©Ÿèƒ½è¦ä»¶ã®ç¶²ç¾…çš„æ¤œè¨¼
  - éæ©Ÿèƒ½ãƒ†ã‚¹ãƒˆ: æ€§èƒ½ãƒ»ã‚»ã‚­ãƒ¥ãƒªãƒ†ã‚£ãƒ»å¯ç”¨æ€§ã®æ¤œè¨¼
  - çµ±åˆãƒ†ã‚¹ãƒˆ: å¤–éƒ¨ã‚·ã‚¹ãƒ†ãƒ ãƒ»APIé€£æºã®æ¤œè¨¼
  - ãƒ¦ãƒ¼ã‚¶ãƒ“ãƒªãƒ†ã‚£ãƒ†ã‚¹ãƒˆ: UXãƒ»æ“ä½œæ€§ã®æ¤œè¨¼
```

## ğŸ§ª ãƒ†ã‚¹ãƒˆãƒ¬ãƒ™ãƒ«ãƒ»åˆ†é¡

### ãƒ†ã‚¹ãƒˆãƒ”ãƒ©ãƒŸãƒƒãƒ‰æ§‹é€ 
```mermaid
graph TD
    A[Manual Testing] --> B[E2E Tests]
    B --> C[Integration Tests]
    C --> D[Unit Tests]
    
    E[Test Coverage] --> F[70% Unit]
    F --> G[20% Integration]
    G --> H[10% E2E]
    
    I[Test Automation] --> J[90% Automated]
    J --> K[10% Manual]
    
    style D fill:#e1f5fe
    style C fill:#f3e5f5
    style B fill:#e8f5e8
    style A fill:#fff3e0
```

### ãƒ†ã‚¹ãƒˆãƒ¬ãƒ™ãƒ«å®šç¾©
```yaml
Level 1 - å˜ä½“ãƒ†ã‚¹ãƒˆ (Unit Tests):
  å¯¾è±¡: å€‹åˆ¥é–¢æ•°ãƒ»ã‚¯ãƒ©ã‚¹ãƒ»ãƒ¢ã‚¸ãƒ¥ãƒ¼ãƒ«
  ç¯„å›²: ãƒ“ã‚¸ãƒã‚¹ãƒ­ã‚¸ãƒƒã‚¯ã€ãƒ‡ãƒ¼ã‚¿å‡¦ç†ã€ãƒ¦ãƒ¼ãƒ†ã‚£ãƒªãƒ†ã‚£é–¢æ•°
  è‡ªå‹•åŒ–ç‡: 100%
  å®Ÿè¡Œé »åº¦: ã‚³ãƒŸãƒƒãƒˆæ¯
  è²¬ä»»è€…: å„é–‹ç™ºè€…
  ãƒ„ãƒ¼ãƒ«: pytest, unittest, Jest

Level 2 - çµ±åˆãƒ†ã‚¹ãƒˆ (Integration Tests):
  å¯¾è±¡: ãƒ¢ã‚¸ãƒ¥ãƒ¼ãƒ«é–“ãƒ»ã‚µãƒ¼ãƒ“ã‚¹é–“é€£æº
  ç¯„å›²: APIé€£æºã€ãƒ‡ãƒ¼ã‚¿ãƒ™ãƒ¼ã‚¹çµ±åˆã€å¤–éƒ¨ã‚µãƒ¼ãƒ“ã‚¹é€£æº
  è‡ªå‹•åŒ–ç‡: 95%
  å®Ÿè¡Œé »åº¦: ãƒ—ãƒ«ãƒªã‚¯ã‚¨ã‚¹ãƒˆæ¯
  è²¬ä»»è€…: é–‹ç™ºãƒãƒ¼ãƒ 
  ãƒ„ãƒ¼ãƒ«: pytest-asyncio, TestContainers

Level 3 - ã‚·ã‚¹ãƒ†ãƒ ãƒ†ã‚¹ãƒˆ (System Tests):
  å¯¾è±¡: ã‚·ã‚¹ãƒ†ãƒ å…¨ä½“ã®æ©Ÿèƒ½ãƒ»æ€§èƒ½
  ç¯„å›²: End-to-Endæ©Ÿèƒ½ã€æ€§èƒ½è¦ä»¶ã€ã‚»ã‚­ãƒ¥ãƒªãƒ†ã‚£
  è‡ªå‹•åŒ–ç‡: 80%
  å®Ÿè¡Œé »åº¦: ãƒ‡ãƒ—ãƒ­ã‚¤å‰
  è²¬ä»»è€…: QAãƒãƒ¼ãƒ 
  ãƒ„ãƒ¼ãƒ«: Playwright, Locust, OWASP ZAP

Level 4 - å—å…¥ãƒ†ã‚¹ãƒˆ (Acceptance Tests):
  å¯¾è±¡: ãƒ“ã‚¸ãƒã‚¹è¦ä»¶ãƒ»ãƒ¦ãƒ¼ã‚¶ãƒ¼ä½“é¨“
  ç¯„å›²: ãƒ¦ãƒ¼ã‚¶ãƒ¼ã‚·ãƒŠãƒªã‚ªã€æ¥­å‹™ãƒ•ãƒ­ãƒ¼
  è‡ªå‹•åŒ–ç‡: 50%
  å®Ÿè¡Œé »åº¦: ãƒªãƒªãƒ¼ã‚¹å‰
  è²¬ä»»è€…: ã‚¹ãƒ†ãƒ¼ã‚¯ãƒ›ãƒ«ãƒ€ãƒ¼ + QA
  ãƒ„ãƒ¼ãƒ«: Cucumber, Manual Testing
```

## ğŸ”¬ æ©Ÿèƒ½ãƒ†ã‚¹ãƒˆä»•æ§˜

### æ–‡æ›¸å–ã‚Šè¾¼ã¿æ©Ÿèƒ½ãƒ†ã‚¹ãƒˆ
```python
# æ–‡æ›¸å–ã‚Šè¾¼ã¿æ©Ÿèƒ½ãƒ†ã‚¹ãƒˆã‚±ãƒ¼ã‚¹
class TestDocumentIngestion:
    
    @pytest.mark.parametrize("file_type,expected_status", [
        ("pdf", "completed"),
        ("html", "completed"),
        ("txt", "completed"),
        ("docx", "completed"),
        ("unsupported.xyz", "failed")
    ])
    async def test_document_processing_by_type(self, file_type, expected_status):
        """æ–‡æ›¸ã‚¿ã‚¤ãƒ—åˆ¥å‡¦ç†ãƒ†ã‚¹ãƒˆ"""
        test_file = create_test_file(file_type)
        
        result = await document_processor.process_document(test_file)
        
        assert result.status == expected_status
        if expected_status == "completed":
            assert result.chunks_count > 0
            assert result.quality_score >= 0.0
            assert len(result.embeddings) == result.chunks_count

    async def test_large_document_processing(self):
        """å¤§å®¹é‡æ–‡æ›¸å‡¦ç†ãƒ†ã‚¹ãƒˆ"""
        # 100MB PDFãƒ•ã‚¡ã‚¤ãƒ«ä½œæˆ
        large_pdf = create_large_test_pdf(size_mb=100)
        
        start_time = time.time()
        result = await document_processor.process_document(large_pdf)
        processing_time = time.time() - start_time
        
        # æ€§èƒ½è¦ä»¶: 1MB/åˆ†ä»¥ä¸‹
        assert processing_time <= 100 * 60  # 100åˆ†ä»¥å†…
        assert result.status == "completed"
        assert result.chunks_count > 100  # é©åˆ‡ãªãƒãƒ£ãƒ³ã‚¯åˆ†å‰²

    async def test_concurrent_document_processing(self):
        """ä¸¦è¡Œå‡¦ç†ãƒ†ã‚¹ãƒˆ"""
        test_files = [create_test_file("pdf") for _ in range(10)]
        
        # 10ãƒ•ã‚¡ã‚¤ãƒ«åŒæ™‚å‡¦ç†
        start_time = time.time()
        results = await asyncio.gather(*[
            document_processor.process_document(file) for file in test_files
        ])
        total_time = time.time() - start_time
        
        # å…¨ã¦æˆåŠŸ
        assert all(r.status == "completed" for r in results)
        # ä¸¦è¡Œå‡¦ç†ã«ã‚ˆã‚Šæ™‚é–“çŸ­ç¸®
        assert total_time < len(test_files) * 60  # é€æ¬¡å‡¦ç†ã‚ˆã‚ŠçŸ­æ™‚é–“

    async def test_duplicate_document_handling(self):
        """é‡è¤‡æ–‡æ›¸å‡¦ç†ãƒ†ã‚¹ãƒˆ"""
        test_file = create_test_file("pdf")
        
        # åˆå›å‡¦ç†
        result1 = await document_processor.process_document(test_file)
        
        # åŒä¸€æ–‡æ›¸å†å‡¦ç†
        result2 = await document_processor.process_document(test_file)
        
        assert result1.document_id == result2.document_id
        assert result2.status == "skipped"  # é‡è¤‡ã¨ã—ã¦å‡¦ç†ã‚¹ã‚­ãƒƒãƒ—
```

### æ¤œç´¢æ©Ÿèƒ½ãƒ†ã‚¹ãƒˆ
```python
class TestSearchFunctionality:
    
    @pytest.fixture
    async def setup_test_knowledge_base(self):
        """ãƒ†ã‚¹ãƒˆç”¨ãƒŠãƒ¬ãƒƒã‚¸ãƒ™ãƒ¼ã‚¹æ§‹ç¯‰"""
        test_documents = [
            {"title": "ãƒ—ãƒ­ã‚¸ã‚§ã‚¯ãƒˆç®¡ç†åŸºç¤", "content": "PMBOKã‚¬ã‚¤ãƒ‰ã«åŸºã¥ããƒ—ãƒ­ã‚¸ã‚§ã‚¯ãƒˆç®¡ç†ã®åŸºæœ¬çš„ãªæ¦‚å¿µã¨æ‰‹æ³•ã«ã¤ã„ã¦èª¬æ˜ã—ã¾ã™ã€‚"},
            {"title": "ãƒªã‚¹ã‚¯ç®¡ç†ãƒ—ãƒ­ã‚»ã‚¹", "content": "ãƒ—ãƒ­ã‚¸ã‚§ã‚¯ãƒˆãƒªã‚¹ã‚¯ã®ç‰¹å®šã€åˆ†æã€å¯¾å¿œã€ç›£è¦–ã®å„ãƒ—ãƒ­ã‚»ã‚¹ã‚’è©³ã—ãè§£èª¬ã—ã¾ã™ã€‚"},
            {"title": "å“è³ªç®¡ç†ã‚·ã‚¹ãƒ†ãƒ ", "content": "ISO 9001ã«åŸºã¥ãå“è³ªãƒãƒã‚¸ãƒ¡ãƒ³ãƒˆã‚·ã‚¹ãƒ†ãƒ ã®æ§‹ç¯‰ã¨é‹ç”¨æ–¹æ³•ã‚’ç´¹ä»‹ã—ã¾ã™ã€‚"}
        ]
        
        for doc in test_documents:
            await self.ingest_test_document(doc)
        
        # ã‚¤ãƒ³ãƒ‡ãƒƒã‚¯ã‚¹æ›´æ–°å®Œäº†ã‚’å¾…æ©Ÿ
        await search_service.refresh_index()

    async def test_semantic_search_accuracy(self, setup_test_knowledge_base):
        """ã‚»ãƒãƒ³ãƒ†ã‚£ãƒƒã‚¯æ¤œç´¢ç²¾åº¦ãƒ†ã‚¹ãƒˆ"""
        test_queries = [
            {
                "query": "ãƒ—ãƒ­ã‚¸ã‚§ã‚¯ãƒˆã®ãƒªã‚¹ã‚¯ã‚’ã©ã†ç®¡ç†ã™ã¹ãã‹ï¼Ÿ",
                "expected_docs": ["ãƒªã‚¹ã‚¯ç®¡ç†ãƒ—ãƒ­ã‚»ã‚¹"],
                "min_similarity": 0.8
            },
            {
                "query": "å“è³ªã‚’ç¢ºä¿ã™ã‚‹ãŸã‚ã®ã‚·ã‚¹ãƒ†ãƒ ã¯ï¼Ÿ",
                "expected_docs": ["å“è³ªç®¡ç†ã‚·ã‚¹ãƒ†ãƒ "],
                "min_similarity": 0.75
            }
        ]
        
        for test_case in test_queries:
            results = await search_service.semantic_search(
                query=test_case["query"],
                limit=5
            )
            
            # æœŸå¾…æ–‡æ›¸ãŒä¸Šä½ã«å­˜åœ¨ã™ã‚‹ã“ã¨ã‚’ç¢ºèª
            top_titles = [r.title for r in results[:3]]
            assert any(expected in top_titles for expected in test_case["expected_docs"])
            
            # æœ€é«˜ã‚¹ã‚³ã‚¢ãŒé–¾å€¤ä»¥ä¸Š
            assert results[0].similarity_score >= test_case["min_similarity"]

    async def test_search_filters(self, setup_test_knowledge_base):
        """æ¤œç´¢ãƒ•ã‚£ãƒ«ã‚¿ãƒªãƒ³ã‚°æ©Ÿèƒ½ãƒ†ã‚¹ãƒˆ"""
        # æ—¥ä»˜ãƒ•ã‚£ãƒ«ã‚¿
        recent_results = await search_service.search(
            query="ç®¡ç†",
            filters={
                "date_range": {
                    "start": "2024-01-01",
                    "end": "2025-12-31"
                }
            }
        )
        
        # å“è³ªã‚¹ã‚³ã‚¢ãƒ•ã‚£ãƒ«ã‚¿
        quality_results = await search_service.search(
            query="ç®¡ç†",
            filters={
                "min_quality_score": 4.0
            }
        )
        
        # ã‚½ãƒ¼ã‚¹ã‚¿ã‚¤ãƒ—ãƒ•ã‚£ãƒ«ã‚¿
        pdf_results = await search_service.search(
            query="ç®¡ç†",
            filters={
                "source_types": ["pdf"]
            }
        )
        
        assert len(recent_results) > 0
        assert all(r.quality_score >= 4.0 for r in quality_results)
        assert all(r.source_type == "pdf" for r in pdf_results)

    async def test_search_performance(self, setup_test_knowledge_base):
        """æ¤œç´¢æ€§èƒ½ãƒ†ã‚¹ãƒˆ"""
        query = "ãƒ—ãƒ­ã‚¸ã‚§ã‚¯ãƒˆç®¡ç†ã®æœ€é©ãªæ‰‹æ³•"
        
        # å˜ç™ºæ¤œç´¢æ€§èƒ½
        start_time = time.time()
        results = await search_service.search(query)
        single_search_time = time.time() - start_time
        
        assert single_search_time <= 3.0  # 3ç§’ä»¥å†…
        assert len(results) > 0
        
        # ä¸¦è¡Œæ¤œç´¢æ€§èƒ½
        queries = [f"{query} {i}" for i in range(10)]
        start_time = time.time()
        concurrent_results = await asyncio.gather(*[
            search_service.search(q) for q in queries
        ])
        concurrent_time = time.time() - start_time
        
        assert concurrent_time <= 5.0  # 5ç§’ä»¥å†…
        assert all(len(r) > 0 for r in concurrent_results)
```

### RAGå›ç­”ç”Ÿæˆæ©Ÿèƒ½ãƒ†ã‚¹ãƒˆ
```python
class TestRAGGeneration:
    
    async def test_answer_generation_quality(self):
        """å›ç­”ç”Ÿæˆå“è³ªãƒ†ã‚¹ãƒˆ"""
        test_questions = [
            {
                "question": "ãƒ—ãƒ­ã‚¸ã‚§ã‚¯ãƒˆãƒªã‚¹ã‚¯ã®ç‰¹å®šæ–¹æ³•ã‚’æ•™ãˆã¦",
                "expected_keywords": ["ãƒªã‚¹ã‚¯ç‰¹å®š", "è­˜åˆ¥", "ãƒã‚§ãƒƒã‚¯ãƒªã‚¹ãƒˆ", "ãƒ–ãƒ¬ã‚¤ãƒ³ã‚¹ãƒˆãƒ¼ãƒŸãƒ³ã‚°"],
                "min_length": 100,
                "max_length": 500
            },
            {
                "question": "å“è³ªç®¡ç†ã§é‡è¦ãªãƒã‚¤ãƒ³ãƒˆã¯ï¼Ÿ",
                "expected_keywords": ["å“è³ªè¨ˆç”»", "å“è³ªä¿è¨¼", "å“è³ªç®¡ç†", "æ”¹å–„"],
                "min_length": 100,
                "max_length": 500
            }
        ]
        
        for test_case in test_questions:
            answer = await rag_service.generate_answer(test_case["question"])
            
            # å›ç­”é•·ãƒã‚§ãƒƒã‚¯
            assert test_case["min_length"] <= len(answer.content) <= test_case["max_length"]
            
            # ã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰å«æœ‰ãƒã‚§ãƒƒã‚¯
            content_lower = answer.content.lower()
            keyword_found = any(
                keyword.lower() in content_lower 
                for keyword in test_case["expected_keywords"]
            )
            assert keyword_found, f"Expected keywords not found in: {answer.content}"
            
            # ã‚½ãƒ¼ã‚¹å¼•ç”¨ãƒã‚§ãƒƒã‚¯
            assert len(answer.sources) > 0
            assert all(source.confidence >= 0.3 for source in answer.sources)

    async def test_answer_generation_performance(self):
        """å›ç­”ç”Ÿæˆæ€§èƒ½ãƒ†ã‚¹ãƒˆ"""
        question = "åŠ¹æœçš„ãªãƒ—ãƒ­ã‚¸ã‚§ã‚¯ãƒˆç®¡ç†ã®ãŸã‚ã«é‡è¦ãªè¦ç´ ã¯ä½•ã§ã™ã‹ï¼Ÿ"
        
        start_time = time.time()
        answer = await rag_service.generate_answer(question)
        response_time = time.time() - start_time
        
        # æ€§èƒ½è¦ä»¶: 5ç§’ä»¥å†…
        assert response_time <= 5.0
        assert len(answer.content) > 50  # æœ€ä½é™ã®å›ç­”é•·
        assert len(answer.sources) > 0   # ã‚½ãƒ¼ã‚¹å¼•ç”¨ã‚ã‚Š

    async def test_multilingual_support(self):
        """å¤šè¨€èªå¯¾å¿œãƒ†ã‚¹ãƒˆ"""
        test_cases = [
            {
                "question": "What are the key principles of project management?",
                "language": "en",
                "expected_lang": "en"
            },
            {
                "question": "ãƒ—ãƒ­ã‚¸ã‚§ã‚¯ãƒˆç®¡ç†ã®ä¸»è¦åŸå‰‡ã¯ä½•ã§ã™ã‹ï¼Ÿ",
                "language": "ja",
                "expected_lang": "ja"
            }
        ]
        
        for test_case in test_cases:
            answer = await rag_service.generate_answer(
                question=test_case["question"],
                language=test_case["language"]
            )
            
            # é©åˆ‡ãªè¨€èªã§å›ç­”ç”Ÿæˆ
            detected_lang = detect_language(answer.content)
            assert detected_lang == test_case["expected_lang"]
            
            # å“è³ªç¢ºä¿
            assert len(answer.content) > 50
            assert len(answer.sources) > 0
```

## âš¡ æ€§èƒ½ãƒ†ã‚¹ãƒˆä»•æ§˜

### è² è·ãƒ†ã‚¹ãƒˆè¨­è¨ˆ
```python
# Locustã‚’ä½¿ç”¨ã—ãŸè² è·ãƒ†ã‚¹ãƒˆã‚·ãƒŠãƒªã‚ª
from locust import HttpUser, task, between

class ERPFTSUserBehavior(HttpUser):
    wait_time = between(1, 5)  # 1-5ç§’å¾…æ©Ÿ
    
    def on_start(self):
        """ãƒ†ã‚¹ãƒˆé–‹å§‹æ™‚ã®åˆæœŸåŒ–"""
        # ãƒ­ã‚°ã‚¤ãƒ³å‡¦ç†
        self.login()
    
    def login(self):
        """ãƒ¦ãƒ¼ã‚¶ãƒ¼ãƒ­ã‚°ã‚¤ãƒ³"""
        response = self.client.post("/auth/login", json={
            "email": "test@example.com",
            "password": "test_password"
        })
        
        if response.status_code == 200:
            token = response.json()["access_token"]
            self.client.headers.update({"Authorization": f"Bearer {token}"})
    
    @task(5)  # é‡ã¿: 5 (æœ€ã‚‚é »ç¹)
    def search_knowledge(self):
        """çŸ¥è­˜æ¤œç´¢ã‚¿ã‚¹ã‚¯"""
        queries = [
            "ãƒ—ãƒ­ã‚¸ã‚§ã‚¯ãƒˆç®¡ç†",
            "ãƒªã‚¹ã‚¯åˆ†æ",
            "å“è³ªä¿è¨¼",
            "ã‚³ã‚¹ãƒˆç®¡ç†",
            "ã‚¹ã‚±ã‚¸ãƒ¥ãƒ¼ãƒ«ç®¡ç†"
        ]
        
        query = random.choice(queries)
        with self.client.get(f"/api/v1/search?q={query}&limit=10", 
                           catch_response=True) as response:
            if response.status_code == 200:
                results = response.json()
                if len(results.get("results", [])) > 0:
                    response.success()
                else:
                    response.failure("No search results returned")
            else:
                response.failure(f"Search failed: {response.status_code}")
    
    @task(2)  # é‡ã¿: 2
    def generate_answer(self):
        """RAGå›ç­”ç”Ÿæˆã‚¿ã‚¹ã‚¯"""
        questions = [
            "ãƒ—ãƒ­ã‚¸ã‚§ã‚¯ãƒˆãƒªã‚¹ã‚¯ã‚’åŠ¹æœçš„ã«ç®¡ç†ã™ã‚‹æ–¹æ³•ã¯ï¼Ÿ",
            "å“è³ªä¿è¨¼ã®ãƒ™ã‚¹ãƒˆãƒ—ãƒ©ã‚¯ãƒ†ã‚£ã‚¹ã‚’æ•™ãˆã¦",
            "ã‚³ã‚¹ãƒˆè¶…éã‚’é˜²ããŸã‚ã®å¯¾ç­–ã¯ï¼Ÿ"
        ]
        
        question = random.choice(questions)
        with self.client.post("/api/v1/rag/answer", 
                            json={"question": question},
                            catch_response=True) as response:
            if response.status_code == 200:
                answer = response.json()
                if len(answer.get("content", "")) > 50:
                    response.success()
                else:
                    response.failure("Answer too short")
            else:
                response.failure(f"Answer generation failed: {response.status_code}")
    
    @task(1)  # é‡ã¿: 1 (æœ€ã‚‚ç¨€)
    def upload_document(self):
        """æ–‡æ›¸ã‚¢ãƒƒãƒ—ãƒ­ãƒ¼ãƒ‰ã‚¿ã‚¹ã‚¯"""
        # ãƒ†ã‚¹ãƒˆæ–‡æ›¸ä½œæˆ
        test_content = "This is a test document for load testing."
        
        files = {"file": ("test.txt", test_content, "text/plain")}
        with self.client.post("/api/v1/documents/upload", 
                            files=files,
                            catch_response=True) as response:
            if response.status_code in [200, 202]:
                response.success()
            else:
                response.failure(f"Upload failed: {response.status_code}")

# è² è·ãƒ†ã‚¹ãƒˆå®Ÿè¡Œè¨­å®š
LOAD_TEST_SCENARIOS = {
    "normal_load": {
        "users": 20,
        "spawn_rate": 2,
        "duration": "10m"
    },
    "peak_load": {
        "users": 50,
        "spawn_rate": 5,
        "duration": "5m"
    },
    "stress_test": {
        "users": 100,
        "spawn_rate": 10,
        "duration": "3m"
    }
}
```

### ãƒ‘ãƒ•ã‚©ãƒ¼ãƒãƒ³ã‚¹ç›£è¦–
```python
class PerformanceMonitor:
    def __init__(self):
        self.metrics = {
            "response_times": [],
            "throughput": [],
            "error_rates": [],
            "resource_usage": []
        }
    
    async def monitor_api_performance(self, endpoint, duration_minutes=10):
        """APIãƒ‘ãƒ•ã‚©ãƒ¼ãƒãƒ³ã‚¹ç›£è¦–"""
        start_time = time.time()
        end_time = start_time + (duration_minutes * 60)
        
        while time.time() < end_time:
            start_request = time.time()
            
            try:
                response = await self.make_test_request(endpoint)
                response_time = time.time() - start_request
                
                self.metrics["response_times"].append({
                    "timestamp": time.time(),
                    "endpoint": endpoint,
                    "response_time": response_time,
                    "status_code": response.status_code
                })
                
            except Exception as e:
                self.metrics["error_rates"].append({
                    "timestamp": time.time(),
                    "endpoint": endpoint,
                    "error": str(e)
                })
            
            await asyncio.sleep(1)  # 1ç§’é–“éš”
    
    def generate_performance_report(self):
        """ãƒ‘ãƒ•ã‚©ãƒ¼ãƒãƒ³ã‚¹ãƒ¬ãƒãƒ¼ãƒˆç”Ÿæˆ"""
        response_times = [m["response_time"] for m in self.metrics["response_times"]]
        
        return {
            "summary": {
                "total_requests": len(response_times),
                "avg_response_time": statistics.mean(response_times),
                "p95_response_time": statistics.quantiles(response_times, n=20)[18],
                "p99_response_time": statistics.quantiles(response_times, n=100)[98],
                "error_rate": len(self.metrics["error_rates"]) / len(response_times) * 100
            },
            "sla_compliance": {
                "response_time_sla": all(rt <= 3.0 for rt in response_times),
                "availability_sla": len(self.metrics["error_rates"]) / len(response_times) <= 0.01
            }
        }
```

## ğŸ”’ ã‚»ã‚­ãƒ¥ãƒªãƒ†ã‚£ãƒ†ã‚¹ãƒˆä»•æ§˜

### èªè¨¼ãƒ»èªå¯ãƒ†ã‚¹ãƒˆ
```python
class TestSecurity:
    
    async def test_authentication_security(self):
        """èªè¨¼ã‚»ã‚­ãƒ¥ãƒªãƒ†ã‚£ãƒ†ã‚¹ãƒˆ"""
        # ä¸æ­£ãƒ­ã‚°ã‚¤ãƒ³è©¦è¡Œ
        invalid_credentials = [
            {"email": "invalid@example.com", "password": "wrong_password"},
            {"email": "test@example.com", "password": ""},
            {"email": "", "password": "password"},
            {"email": "test@example.com", "password": "wrong"},
        ]
        
        for cred in invalid_credentials:
            response = await auth_client.login(cred["email"], cred["password"])
            assert response.status_code == 401
            assert "access_token" not in response.json()
        
        # ãƒ–ãƒ«ãƒ¼ãƒˆãƒ•ã‚©ãƒ¼ã‚¹æ”»æ’ƒå¯¾ç­–
        for i in range(10):  # 10å›é€£ç¶šå¤±æ•—
            response = await auth_client.login("test@example.com", "wrong_password")
            assert response.status_code == 401
        
        # ã‚¢ã‚«ã‚¦ãƒ³ãƒˆãƒ­ãƒƒã‚¯ç¢ºèª
        response = await auth_client.login("test@example.com", "correct_password")
        assert response.status_code == 429  # Too Many Requests

    async def test_authorization_controls(self):
        """èªå¯åˆ¶å¾¡ãƒ†ã‚¹ãƒˆ"""
        # å„ãƒ­ãƒ¼ãƒ«åˆ¥ã‚¢ã‚¯ã‚»ã‚¹ãƒ†ã‚¹ãƒˆ
        test_scenarios = [
            {
                "role": "guest",
                "allowed_endpoints": ["/api/v1/search"],
                "forbidden_endpoints": ["/api/v1/admin", "/api/v1/documents/upload"]
            },
            {
                "role": "viewer", 
                "allowed_endpoints": ["/api/v1/search", "/api/v1/feedback"],
                "forbidden_endpoints": ["/api/v1/admin", "/api/v1/documents/upload"]
            },
            {
                "role": "admin",
                "allowed_endpoints": ["/api/v1/search", "/api/v1/admin", "/api/v1/documents/upload"],
                "forbidden_endpoints": []
            }
        ]
        
        for scenario in test_scenarios:
            token = await create_test_token(role=scenario["role"])
            headers = {"Authorization": f"Bearer {token}"}
            
            # è¨±å¯ã•ã‚ŒãŸã‚¨ãƒ³ãƒ‰ãƒã‚¤ãƒ³ãƒˆ
            for endpoint in scenario["allowed_endpoints"]:
                response = await test_client.get(endpoint, headers=headers)
                assert response.status_code != 403, f"Access denied to allowed endpoint: {endpoint}"
            
            # ç¦æ­¢ã•ã‚ŒãŸã‚¨ãƒ³ãƒ‰ãƒã‚¤ãƒ³ãƒˆ
            for endpoint in scenario["forbidden_endpoints"]:
                response = await test_client.get(endpoint, headers=headers)
                assert response.status_code == 403, f"Access granted to forbidden endpoint: {endpoint}"

    async def test_input_validation_security(self):
        """å…¥åŠ›æ¤œè¨¼ã‚»ã‚­ãƒ¥ãƒªãƒ†ã‚£ãƒ†ã‚¹ãƒˆ"""
        # SQLã‚¤ãƒ³ã‚¸ã‚§ã‚¯ã‚·ãƒ§ãƒ³æ”»æ’ƒãƒ†ã‚¹ãƒˆ
        sql_injection_payloads = [
            "'; DROP TABLE users; --",
            "' OR '1'='1",
            "admin'--",
            "' UNION SELECT * FROM users --"
        ]
        
        for payload in sql_injection_payloads:
            response = await search_client.search(query=payload)
            # SQLã‚¨ãƒ©ãƒ¼ãŒè¿”ã•ã‚Œãªã„ã“ã¨ã‚’ç¢ºèª
            assert "SQL" not in response.text.upper()
            assert "ERROR" not in response.text.upper()
        
        # XSSãƒ†ã‚¹ãƒˆ
        xss_payloads = [
            "<script>alert('XSS')</script>",
            "javascript:alert('XSS')",
            "<img src=x onerror=alert('XSS')>"
        ]
        
        for payload in xss_payloads:
            response = await search_client.search(query=payload)
            # ã‚¹ã‚¯ãƒªãƒ—ãƒˆãŒã‚¨ã‚¹ã‚±ãƒ¼ãƒ—ã•ã‚Œã¦ã„ã‚‹ã“ã¨ã‚’ç¢ºèª
            assert "<script>" not in response.text
            assert "javascript:" not in response.text

    async def test_data_encryption(self):
        """ãƒ‡ãƒ¼ã‚¿æš—å·åŒ–ãƒ†ã‚¹ãƒˆ"""
        # è»¢é€ä¸­æš—å·åŒ–ç¢ºèª
        response = await test_client.get("/api/v1/search?q=test")
        
        # HTTPSé€šä¿¡ç¢ºèª
        assert response.url.scheme == "https"
        
        # ã‚»ã‚­ãƒ¥ãƒªãƒ†ã‚£ãƒ˜ãƒƒãƒ€ãƒ¼ç¢ºèª
        security_headers = [
            "Strict-Transport-Security",
            "X-Content-Type-Options", 
            "X-Frame-Options",
            "X-XSS-Protection"
        ]
        
        for header in security_headers:
            assert header in response.headers
        
        # ä¿å­˜æ™‚æš—å·åŒ–ç¢ºèªï¼ˆãƒ‡ãƒ¼ã‚¿ãƒ™ãƒ¼ã‚¹ï¼‰
        sensitive_data = "sensitive_test_data"
        document_id = await storage_service.store_document(sensitive_data)
        
        # ç›´æ¥ãƒ‡ãƒ¼ã‚¿ãƒ™ãƒ¼ã‚¹ã‹ã‚‰å–å¾—ï¼ˆæš—å·åŒ–ã•ã‚Œã¦ã„ã‚‹ã¯ãšï¼‰
        raw_data = await db.fetch_raw_document(document_id)
        assert raw_data != sensitive_data  # æš—å·åŒ–ã•ã‚Œã¦ã„ã‚‹
        
        # æ­£è¦ã®æ–¹æ³•ã§å–å¾—ï¼ˆå¾©å·åŒ–ã•ã‚Œã‚‹ï¼‰
        decrypted_data = await storage_service.get_document(document_id)
        assert decrypted_data == sensitive_data  # æ­£ã—ãå¾©å·åŒ–
```

## ğŸ”„ è‡ªå‹•ãƒ†ã‚¹ãƒˆåŸºç›¤

### CI/CDãƒ†ã‚¹ãƒˆãƒ‘ã‚¤ãƒ—ãƒ©ã‚¤ãƒ³
```yaml
# .github/workflows/test.yml
name: Comprehensive Test Suite

on:
  push:
    branches: [ main, develop ]
  pull_request:
    branches: [ main ]

jobs:
  unit-tests:
    runs-on: ubuntu-latest
    strategy:
      matrix:
        python-version: [3.11, 3.12]
    
    steps:
    - uses: actions/checkout@v3
    
    - name: Set up Python
      uses: actions/setup-python@v4
      with:
        python-version: ${{ matrix.python-version }}
    
    - name: Install dependencies
      run: |
        pip install -r requirements-test.txt
        pip install -e .
    
    - name: Run unit tests
      run: |
        pytest tests/unit/ -v --cov=erpfts --cov-report=xml
    
    - name: Upload coverage reports
      uses: codecov/codecov-action@v3
      with:
        file: ./coverage.xml

  integration-tests:
    runs-on: ubuntu-latest
    needs: unit-tests
    
    services:
      postgres:
        image: postgres:15
        env:
          POSTGRES_PASSWORD: test_password
          POSTGRES_DB: erpfts_test
        options: >-
          --health-cmd pg_isready
          --health-interval 10s
          --health-timeout 5s
          --health-retries 5
      
      redis:
        image: redis:7-alpine
        options: >-
          --health-cmd "redis-cli ping"
          --health-interval 10s
          --health-timeout 5s
          --health-retries 5
    
    steps:
    - uses: actions/checkout@v3
    
    - name: Set up Python
      uses: actions/setup-python@v4
      with:
        python-version: 3.11
    
    - name: Install dependencies
      run: |
        pip install -r requirements-test.txt
        pip install -e .
    
    - name: Run integration tests
      env:
        DATABASE_URL: postgresql://postgres:test_password@localhost:5432/erpfts_test
        REDIS_URL: redis://localhost:6379/0
        TEST_ENV: integration
      run: |
        pytest tests/integration/ -v --tb=short

  security-tests:
    runs-on: ubuntu-latest
    needs: integration-tests
    
    steps:
    - uses: actions/checkout@v3
    
    - name: Run security scan
      uses: securecodewarrior/github-action-add-sarif@v1
      with:
        sarif-file: security-scan-results.sarif
    
    - name: OWASP ZAP security scan
      uses: zaproxy/action-full-scan@v0.4.0
      with:
        target: 'http://localhost:8000'

  performance-tests:
    runs-on: ubuntu-latest
    needs: integration-tests
    if: github.event_name == 'push' && github.ref == 'refs/heads/main'
    
    steps:
    - uses: actions/checkout@v3
    
    - name: Set up Python
      uses: actions/setup-python@v4
      with:
        python-version: 3.11
    
    - name: Install dependencies
      run: |
        pip install locust pytest-benchmark
    
    - name: Run performance tests
      run: |
        locust -f tests/performance/load_test.py --headless -u 10 -r 2 -t 5m --html performance-report.html
    
    - name: Upload performance report
      uses: actions/upload-artifact@v3
      with:
        name: performance-report
        path: performance-report.html
```

### ãƒ†ã‚¹ãƒˆãƒ‡ãƒ¼ã‚¿ç®¡ç†
```python
# ãƒ†ã‚¹ãƒˆãƒ‡ãƒ¼ã‚¿ãƒ•ã‚¡ã‚¯ãƒˆãƒª
class TestDataFactory:
    
    @staticmethod
    def create_test_user(role="viewer", **kwargs):
        """ãƒ†ã‚¹ãƒˆãƒ¦ãƒ¼ã‚¶ãƒ¼ä½œæˆ"""
        default_data = {
            "email": f"test_{uuid.uuid4().hex[:8]}@example.com",
            "display_name": "Test User",
            "role": role,
            "is_active": True
        }
        default_data.update(kwargs)
        return default_data
    
    @staticmethod
    def create_test_document(**kwargs):
        """ãƒ†ã‚¹ãƒˆæ–‡æ›¸ä½œæˆ"""
        default_data = {
            "title": f"Test Document {uuid.uuid4().hex[:8]}",
            "content": "This is a test document for automated testing purposes.",
            "author": "Test Author",
            "document_type": "article",
            "language": "ja",
            "quality_score": 4.0
        }
        default_data.update(kwargs)
        return default_data
    
    @staticmethod
    def create_test_search_query(**kwargs):
        """ãƒ†ã‚¹ãƒˆæ¤œç´¢ã‚¯ã‚¨ãƒªä½œæˆ"""
        default_queries = [
            "ãƒ—ãƒ­ã‚¸ã‚§ã‚¯ãƒˆç®¡ç†ã®åŸºæœ¬",
            "ãƒªã‚¹ã‚¯åˆ†ææ‰‹æ³•",
            "å“è³ªä¿è¨¼ãƒ—ãƒ­ã‚»ã‚¹",
            "ã‚³ã‚¹ãƒˆç®¡ç†æŠ€æ³•"
        ]
        
        default_data = {
            "query": random.choice(default_queries),
            "query_type": "semantic",
            "filters": {},
            "limit": 10
        }
        default_data.update(kwargs)
        return default_data

# ãƒ†ã‚¹ãƒˆãƒ‡ãƒ¼ã‚¿ãƒ™ãƒ¼ã‚¹ç®¡ç†
class TestDatabaseManager:
    
    async def setup_test_db(self):
        """ãƒ†ã‚¹ãƒˆãƒ‡ãƒ¼ã‚¿ãƒ™ãƒ¼ã‚¹åˆæœŸåŒ–"""
        # ã‚¹ã‚­ãƒ¼ãƒä½œæˆ
        await self.create_tables()
        
        # åŸºæœ¬ãƒ†ã‚¹ãƒˆãƒ‡ãƒ¼ã‚¿æŠ•å…¥
        await self.seed_test_data()
    
    async def cleanup_test_db(self):
        """ãƒ†ã‚¹ãƒˆãƒ‡ãƒ¼ã‚¿ãƒ™ãƒ¼ã‚¹ã‚¯ãƒªãƒ¼ãƒ³ã‚¢ãƒƒãƒ—"""
        # å…¨ãƒ†ãƒ¼ãƒ–ãƒ«ãƒˆãƒ©ãƒ³ã‚±ãƒ¼ãƒˆ
        await self.truncate_all_tables()
    
    async def seed_test_data(self):
        """åŸºæœ¬ãƒ†ã‚¹ãƒˆãƒ‡ãƒ¼ã‚¿æŠ•å…¥"""
        # ãƒ†ã‚¹ãƒˆãƒ¦ãƒ¼ã‚¶ãƒ¼ä½œæˆ
        test_users = [
            TestDataFactory.create_test_user(role="admin"),
            TestDataFactory.create_test_user(role="editor"),
            TestDataFactory.create_test_user(role="viewer"),
            TestDataFactory.create_test_user(role="guest")
        ]
        
        for user_data in test_users:
            await self.create_user(user_data)
        
        # ãƒ†ã‚¹ãƒˆæ–‡æ›¸ä½œæˆ
        test_docs = [
            TestDataFactory.create_test_document(title="PMBOK Guide"),
            TestDataFactory.create_test_document(title="BABOK Guide"),
            TestDataFactory.create_test_document(title="ISO 9001 Standard")
        ]
        
        for doc_data in test_docs:
            await self.create_document(doc_data)
```

## ğŸ“Š ãƒ†ã‚¹ãƒˆçµæœç®¡ç†ãƒ»ãƒ¬ãƒãƒ¼ãƒˆ

### ãƒ†ã‚¹ãƒˆå®Ÿè¡Œãƒ¬ãƒãƒ¼ãƒˆ
```python
class TestReportGenerator:
    
    def __init__(self):
        self.test_results = []
        self.performance_metrics = []
        self.coverage_data = {}
    
    def generate_comprehensive_report(self):
        """åŒ…æ‹¬çš„ãƒ†ã‚¹ãƒˆãƒ¬ãƒãƒ¼ãƒˆç”Ÿæˆ"""
        report = {
            "execution_summary": self._generate_execution_summary(),
            "coverage_analysis": self._generate_coverage_analysis(),
            "performance_analysis": self._generate_performance_analysis(),
            "quality_metrics": self._generate_quality_metrics(),
            "recommendations": self._generate_recommendations()
        }
        
        return report
    
    def _generate_execution_summary(self):
        """å®Ÿè¡Œã‚µãƒãƒªãƒ¼ç”Ÿæˆ"""
        total_tests = len(self.test_results)
        passed_tests = len([r for r in self.test_results if r.status == "passed"])
        failed_tests = len([r for r in self.test_results if r.status == "failed"])
        skipped_tests = len([r for r in self.test_results if r.status == "skipped"])
        
        return {
            "total_tests": total_tests,
            "passed": passed_tests,
            "failed": failed_tests,
            "skipped": skipped_tests,
            "pass_rate": (passed_tests / total_tests * 100) if total_tests > 0 else 0,
            "execution_time": sum(r.duration for r in self.test_results)
        }
    
    def _generate_coverage_analysis(self):
        """ã‚«ãƒãƒ¬ãƒƒã‚¸åˆ†æç”Ÿæˆ"""
        return {
            "overall_coverage": self.coverage_data.get("overall", 0),
            "module_coverage": self.coverage_data.get("modules", {}),
            "uncovered_lines": self.coverage_data.get("uncovered", []),
            "coverage_trend": self._calculate_coverage_trend()
        }
    
    def _generate_quality_metrics(self):
        """å“è³ªãƒ¡ãƒˆãƒªã‚¯ã‚¹ç”Ÿæˆ"""
        return {
            "defect_density": self._calculate_defect_density(),
            "test_effectiveness": self._calculate_test_effectiveness(),
            "automation_rate": self._calculate_automation_rate(),
            "mean_time_to_detect": self._calculate_mttd(),
            "mean_time_to_repair": self._calculate_mttr()
        }

# ãƒ†ã‚¹ãƒˆå“è³ªãƒ€ãƒƒã‚·ãƒ¥ãƒœãƒ¼ãƒ‰
QUALITY_DASHBOARD_CONFIG = {
    "kpi_targets": {
        "test_coverage": 80,           # 80%ä»¥ä¸Š
        "pass_rate": 95,               # 95%ä»¥ä¸Š
        "automation_rate": 90,         # 90%ä»¥ä¸Š
        "defect_escape_rate": 5,       # 5%ä»¥ä¸‹
        "mean_execution_time": 300     # 5åˆ†ä»¥å†…
    },
    "alerts": {
        "coverage_drop": "5%",         # ã‚«ãƒãƒ¬ãƒƒã‚¸5%ä½ä¸‹ã§ã‚¢ãƒ©ãƒ¼ãƒˆ
        "failure_spike": "10%",        # å¤±æ•—ç‡10%ä¸Šæ˜‡ã§ã‚¢ãƒ©ãƒ¼ãƒˆ
        "performance_degradation": "20%"  # æ€§èƒ½20%åŠ£åŒ–ã§ã‚¢ãƒ©ãƒ¼ãƒˆ
    }
}
```

## ğŸ¤– Implementation Notes for AI

### Critical Implementation Paths
1. **ãƒ†ã‚¹ãƒˆãƒ•ãƒ¬ãƒ¼ãƒ ãƒ¯ãƒ¼ã‚¯æ§‹ç¯‰**: pytest + async/await â†’ éåŒæœŸãƒ†ã‚¹ãƒˆå¯¾å¿œ
2. **è‡ªå‹•åŒ–ãƒ‘ã‚¤ãƒ—ãƒ©ã‚¤ãƒ³**: GitHub Actions + Docker â†’ CI/CDçµ±åˆ
3. **æ€§èƒ½ãƒ†ã‚¹ãƒˆ**: Locust + monitoring â†’ ç¶™ç¶šçš„æ€§èƒ½æ¤œè¨¼
4. **ã‚»ã‚­ãƒ¥ãƒªãƒ†ã‚£ãƒ†ã‚¹ãƒˆ**: OWASP ZAP + custom scripts â†’ è„†å¼±æ€§æ¤œå‡º

### Key Dependencies
- **ãƒ†ã‚¹ãƒˆãƒ•ãƒ¬ãƒ¼ãƒ ãƒ¯ãƒ¼ã‚¯**: pytest, pytest-asyncio, pytest-cov
- **æ€§èƒ½ãƒ†ã‚¹ãƒˆ**: locust, pytest-benchmark
- **ã‚»ã‚­ãƒ¥ãƒªãƒ†ã‚£ãƒ†ã‚¹ãƒˆ**: bandit, safety, OWASP ZAP
- **ãƒ¢ãƒƒã‚¯ãƒ»ã‚¹ã‚¿ãƒ–**: pytest-mock, responses, aioresponses

### Testing Strategy
- **TDD**: Test-Driven Development ã«ã‚ˆã‚‹å“è³ªç¢ºä¿
- **ç¶™ç¶šçš„ãƒ†ã‚¹ãƒˆ**: ã‚³ãƒŸãƒƒãƒˆæ¯ã®è‡ªå‹•ãƒ†ã‚¹ãƒˆå®Ÿè¡Œ
- **ãƒªã‚¹ã‚¯ãƒ™ãƒ¼ã‚¹ãƒ†ã‚¹ãƒˆ**: é‡è¦åº¦ãƒ»å½±éŸ¿åº¦ã«ã‚ˆã‚‹å„ªå…ˆåº¦ä»˜ã‘
- **æ€§èƒ½å›å¸°ãƒ†ã‚¹ãƒˆ**: æ€§èƒ½åŠ£åŒ–ã®æ—©æœŸæ¤œå‡º

### Common Pitfalls
- **ãƒ†ã‚¹ãƒˆãƒ‡ãƒ¼ã‚¿ç®¡ç†**: ãƒ†ã‚¹ãƒˆé–“ã®ä¾å­˜é–¢ä¿‚ãƒ»ãƒ‡ãƒ¼ã‚¿ç«¶åˆ
- **éåŒæœŸãƒ†ã‚¹ãƒˆ**: async/await ã®é©åˆ‡ãªä½¿ç”¨
- **å¤–éƒ¨ä¾å­˜**: APIãƒ»DBæ¥ç¶šã®ãƒ¢ãƒƒã‚¯åŒ–ä¸å‚™
- **æ€§èƒ½ãƒ†ã‚¹ãƒˆ**: å®Ÿç’°å¢ƒã¨ã®ä¹–é›¢

### å®Ÿè£…å„ªå…ˆé †ä½
1. **Phase 1**: å˜ä½“ãƒ†ã‚¹ãƒˆã€åŸºæœ¬çµ±åˆãƒ†ã‚¹ãƒˆ
2. **Phase 2**: æ€§èƒ½ãƒ†ã‚¹ãƒˆã€ã‚»ã‚­ãƒ¥ãƒªãƒ†ã‚£ãƒ†ã‚¹ãƒˆ
3. **Phase 3**: E2Eãƒ†ã‚¹ãƒˆã€å—å…¥ãƒ†ã‚¹ãƒˆè‡ªå‹•åŒ–

---

**Version**: 1.0.0 | **Last Updated**: 2025-01-21 | **Next Review**: 2025-02-21